{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-30 00:01:30.745742: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0\n",
      "2022-04-30 00:01:33.445487: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-04-30 00:01:33.454575: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcuda.so.1\n",
      "2022-04-30 00:01:33.900466: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1733] Found device 0 with properties: \n",
      "pciBusID: 0000:57:00.0 name: NVIDIA GeForce RTX 3090 computeCapability: 8.6\n",
      "coreClock: 1.695GHz coreCount: 82 deviceMemorySize: 23.70GiB deviceMemoryBandwidth: 871.81GiB/s\n",
      "2022-04-30 00:01:33.901245: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1733] Found device 1 with properties: \n",
      "pciBusID: 0000:ce:00.0 name: NVIDIA GeForce RTX 3090 computeCapability: 8.6\n",
      "coreClock: 1.695GHz coreCount: 82 deviceMemorySize: 23.70GiB deviceMemoryBandwidth: 871.81GiB/s\n",
      "2022-04-30 00:01:33.901282: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0\n",
      "2022-04-30 00:01:33.904964: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcublas.so.11\n",
      "2022-04-30 00:01:33.905065: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcublasLt.so.11\n",
      "2022-04-30 00:01:33.906455: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcufft.so.10\n",
      "2022-04-30 00:01:33.906817: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcurand.so.10\n",
      "2022-04-30 00:01:33.908069: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcusolver.so.11\n",
      "2022-04-30 00:01:33.909054: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcusparse.so.11\n",
      "2022-04-30 00:01:33.909264: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudnn.so.8\n",
      "2022-04-30 00:01:33.912094: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1871] Adding visible gpu devices: 0, 1\n",
      "2022-04-30 00:01:33.912140: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device mapping:\n",
      "/job:localhost/replica:0/task:0/device:GPU:0 -> device: 0, name: NVIDIA GeForce RTX 3090, pci bus id: 0000:57:00.0, compute capability: 8.6\n",
      "/job:localhost/replica:0/task:0/device:GPU:1 -> device: 1, name: NVIDIA GeForce RTX 3090, pci bus id: 0000:ce:00.0, compute capability: 8.6\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-30 00:01:34.950607: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1258] Device interconnect StreamExecutor with strength 1 edge matrix:\n",
      "2022-04-30 00:01:34.950653: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1264]      0 1 \n",
      "2022-04-30 00:01:34.950663: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1277] 0:   N N \n",
      "2022-04-30 00:01:34.950669: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1277] 1:   N N \n",
      "2022-04-30 00:01:34.956237: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1418] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 22308 MB memory) -> physical GPU (device: 0, name: NVIDIA GeForce RTX 3090, pci bus id: 0000:57:00.0, compute capability: 8.6)\n",
      "2022-04-30 00:01:34.985306: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1418] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:1 with 22308 MB memory) -> physical GPU (device: 1, name: NVIDIA GeForce RTX 3090, pci bus id: 0000:ce:00.0, compute capability: 8.6)\n"
     ]
    }
   ],
   "source": [
    "# Import packages\n",
    "from tensorflow.python.keras import layers\n",
    "import numpy as np\n",
    "from tensorflow.python.keras.callbacks import TensorBoard\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import os.path\n",
    "import glob\n",
    "import os\n",
    "import cv2\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import tensorflow as tf\n",
    "from tensorflow.compat.v1.keras.backend import set_session\n",
    "config = tf.compat.v1.ConfigProto()\n",
    "config.gpu_options.allow_growth = True  # dynamically grow the memory used on the GPU\n",
    "config.log_device_placement = True  # to log device placement (on which device the operation ran)\n",
    "sess = tf.compat.v1.Session(config=config)\n",
    "set_session(sess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def convertjpg(jpgfile,outdir,width=64,height=64):\n",
    "    img=Image.open(jpgfile)   \n",
    "    new_img=img.resize((width,height),Image.BILINEAR)   \n",
    "    new_img.save(os.path.join(outdir,os.path.basename(jpgfile)))\n",
    "for jpgfile in glob.glob('/root/autodl-tmp/thispersondoesnotexist.10k/*.jpg'):\n",
    "    convertjpg(jpgfile,'/root/autodl-tmp/thispersondoesnotexist.10k')\n",
    "for jpgfile in glob.glob('/root/autodl-tmp/0/*.jpg'):\n",
    "    convertjpg(jpgfile,'/root/autodl-tmp/0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def getFileName(filepath):\n",
    "    path_list = []\n",
    "    pathList = os.listdir(filepath)\n",
    "    for allFile in pathList:\n",
    "        everyFile = os.path.join(filepath, allFile)\n",
    "        path_list.append(everyFile)\n",
    "    return path_list\n",
    "training_path = getFileName('/root/autodl-tmp/thispersondoesnotexist.10k')\n",
    "testing_path = getFileName('/root/autodl-tmp/0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "training_path = training_path[:10000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000\n",
      "10000\n"
     ]
    }
   ],
   "source": [
    "print(len(training_path))\n",
    "print(len(testing_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "testing_img_path = training_path[:800] + testing_path[:200]\n",
    "training_img_path = training_path[800:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "test_y = 800*[0] + 200*[1]\n",
    "test_y = np.array(test_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(9200, 64, 64, 3)\n",
      "(1000, 64, 64, 3)\n"
     ]
    }
   ],
   "source": [
    "train = np.array([cv2.imdecode(np.fromfile(path,dtype=np.uint8),-1)/ 255. for path in training_img_path])\n",
    "train = train.reshape(9200,64,64,3)\n",
    "print(train.shape)\n",
    "test = np.array([cv2.imdecode(np.fromfile(path,dtype=np.uint8),-1)/ 255. for path in testing_img_path])\n",
    "test = test.reshape(1000,64,64,3)\n",
    "print(test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            [(None, 64, 64, 3)]  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d (Conv2D)                 (None, 64, 64, 16)   448         input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D)    (None, 32, 32, 16)   0           conv2d[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_1 (Conv2D)               (None, 32, 32, 8)    1160        max_pooling2d[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2D)  (None, 16, 16, 8)    0           conv2d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_2 (Conv2D)               (None, 16, 16, 8)    584         max_pooling2d_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2D)  (None, 8, 8, 8)      0           conv2d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_3 (Conv2D)               (None, 8, 8, 4)      292         max_pooling2d_2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2D)  (None, 4, 4, 4)      0           conv2d_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "flatten (Flatten)               (None, 64)           0           max_pooling2d_3[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, 64)           4160        flatten[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 32)           2080        dense[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 16)           528         dense_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_3 (Dense)                 (None, 8)            136         dense_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_4 (Dense)                 (None, 4)            36          dense_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_5 (Dense)                 (None, 4)            20          dense_4[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_6 (Dense)                 (None, 16)           80          dense_5[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_7 (Dense)                 (None, 32)           544         dense_6[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_8 (Dense)                 (None, 64)           2112        dense_7[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "tf.compat.v1.shape (TFOpLambda) (4,)                 0           max_pooling2d_3[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "tf.reshape (TFOpLambda)         (None, 4, 4, 4)      0           dense_8[0][0]                    \n",
      "                                                                 tf.compat.v1.shape[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_4 (Conv2D)               (None, 4, 4, 4)      148         tf.reshape[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "up_sampling2d (UpSampling2D)    (None, 8, 8, 4)      0           conv2d_4[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_5 (Conv2D)               (None, 8, 8, 8)      296         up_sampling2d[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "up_sampling2d_1 (UpSampling2D)  (None, 16, 16, 8)    0           conv2d_5[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_6 (Conv2D)               (None, 16, 16, 8)    584         up_sampling2d_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "up_sampling2d_2 (UpSampling2D)  (None, 32, 32, 8)    0           conv2d_6[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_7 (Conv2D)               (None, 32, 32, 16)   1168        up_sampling2d_2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "up_sampling2d_3 (UpSampling2D)  (None, 64, 64, 16)   0           conv2d_7[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_8 (Conv2D)               (None, 64, 64, 3)    435         up_sampling2d_3[0][0]            \n",
      "==================================================================================================\n",
      "Total params: 14,811\n",
      "Trainable params: 14,811\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-30 00:01:45.378987: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1733] Found device 0 with properties: \n",
      "pciBusID: 0000:57:00.0 name: NVIDIA GeForce RTX 3090 computeCapability: 8.6\n",
      "coreClock: 1.695GHz coreCount: 82 deviceMemorySize: 23.70GiB deviceMemoryBandwidth: 871.81GiB/s\n",
      "2022-04-30 00:01:45.379800: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1733] Found device 1 with properties: \n",
      "pciBusID: 0000:ce:00.0 name: NVIDIA GeForce RTX 3090 computeCapability: 8.6\n",
      "coreClock: 1.695GHz coreCount: 82 deviceMemorySize: 23.70GiB deviceMemoryBandwidth: 871.81GiB/s\n",
      "2022-04-30 00:01:45.383983: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1871] Adding visible gpu devices: 0, 1\n",
      "2022-04-30 00:01:45.385239: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1733] Found device 0 with properties: \n",
      "pciBusID: 0000:57:00.0 name: NVIDIA GeForce RTX 3090 computeCapability: 8.6\n",
      "coreClock: 1.695GHz coreCount: 82 deviceMemorySize: 23.70GiB deviceMemoryBandwidth: 871.81GiB/s\n",
      "2022-04-30 00:01:45.385955: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1733] Found device 1 with properties: \n",
      "pciBusID: 0000:ce:00.0 name: NVIDIA GeForce RTX 3090 computeCapability: 8.6\n",
      "coreClock: 1.695GHz coreCount: 82 deviceMemorySize: 23.70GiB deviceMemoryBandwidth: 871.81GiB/s\n",
      "2022-04-30 00:01:45.388726: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1871] Adding visible gpu devices: 0, 1\n",
      "2022-04-30 00:01:45.388774: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1258] Device interconnect StreamExecutor with strength 1 edge matrix:\n",
      "2022-04-30 00:01:45.388779: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1264]      0 1 \n",
      "2022-04-30 00:01:45.388784: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1277] 0:   N N \n",
      "2022-04-30 00:01:45.388787: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1277] 1:   N N \n",
      "2022-04-30 00:01:45.390920: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1418] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 22308 MB memory) -> physical GPU (device: 0, name: NVIDIA GeForce RTX 3090, pci bus id: 0000:57:00.0, compute capability: 8.6)\n",
      "2022-04-30 00:01:45.392366: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1418] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:1 with 22308 MB memory) -> physical GPU (device: 1, name: NVIDIA GeForce RTX 3090, pci bus id: 0000:ce:00.0, compute capability: 8.6)\n"
     ]
    }
   ],
   "source": [
    "import tensorflow.python.keras as keras\n",
    "from tensorflow.python.keras import layers\n",
    "\n",
    "input_img = keras.Input(shape=(64, 64, 3))\n",
    "\n",
    "x = layers.Conv2D(16, (3, 3), activation='relu', padding='same')(input_img)\n",
    "x = layers.MaxPooling2D((2, 2), padding='same')(x)\n",
    "x = layers.Conv2D(8, (3, 3), activation='relu', padding='same')(x)\n",
    "x = layers.MaxPooling2D((2, 2), padding='same')(x)\n",
    "x = layers.Conv2D(8, (3, 3), activation='relu', padding='same')(x)\n",
    "x = layers.MaxPooling2D((2, 2), padding='same')(x)\n",
    "x = layers.Conv2D(4, (3, 3), activation='relu', padding='same')(x)\n",
    "x = layers.MaxPooling2D((2, 2), padding='same')(x)\n",
    "conv_shape = tf.shape(x)\n",
    "x = layers.Flatten()(x)\n",
    "x = layers.Dense(64, activation='relu')(x)\n",
    "x = layers.Dense(32, activation='relu')(x)\n",
    "x = layers.Dense(16, activation='relu')(x)\n",
    "x = layers.Dense(8, activation='relu')(x)\n",
    "encoded = layers.Dense(4, activation='relu')(x)\n",
    "x = layers.Dense(4, activation='relu')(encoded)\n",
    "x = layers.Dense(16, activation='relu')(x)\n",
    "x = layers.Dense(32, activation='relu')(x)\n",
    "x = layers.Dense(64, activation='relu')(x)\n",
    "x = tf.reshape(x, conv_shape)\n",
    "x = layers.Conv2D(4, (3, 3), activation='relu', padding='same')(x)\n",
    "x = layers.UpSampling2D((2, 2))(x)\n",
    "x = layers.Conv2D(8, (3, 3), activation='relu', padding='same')(x)\n",
    "x = layers.UpSampling2D((2, 2))(x)\n",
    "x = layers.Conv2D(8, (3, 3), activation='relu', padding='same')(x)\n",
    "x = layers.UpSampling2D((2, 2))(x)\n",
    "x = layers.Conv2D(16, (3, 3), activation='relu', padding='same')(x) \n",
    "x = layers.UpSampling2D((2, 2))(x)\n",
    "decoded = layers.Conv2D(3, (3, 3), activation='sigmoid', padding='same')(x)\n",
    "autoencoder = keras.Model(input_img, decoded)\n",
    "autoencoder.compile(optimizer='adam', loss='binary_crossentropy')\n",
    "autoencoder.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-30 00:01:46.796106: I tensorflow/core/profiler/lib/profiler_session.cc:126] Profiler session initializing.\n",
      "2022-04-30 00:01:46.796153: I tensorflow/core/profiler/lib/profiler_session.cc:141] Profiler session started.\n",
      "2022-04-30 00:01:46.796258: I tensorflow/core/profiler/internal/gpu/cupti_tracer.cc:1611] Profiler found 2 GPUs\n",
      "2022-04-30 00:01:46.798540: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcupti.so.11.2\n",
      "2022-04-30 00:01:47.028823: I tensorflow/core/profiler/lib/profiler_session.cc:159] Profiler session tear down.\n",
      "2022-04-30 00:01:47.035751: I tensorflow/core/profiler/internal/gpu/cupti_tracer.cc:1743] CUPTI activity buffer flushed\n",
      "2022-04-30 00:01:48.702866: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:176] None of the MLIR Optimization Passes are enabled (registered 2)\n",
      "2022-04-30 00:01:48.707905: I tensorflow/core/platform/profile_utils/cpu_utils.cc:114] CPU Frequency: 2000000000 Hz\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-30 00:01:50.045529: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudnn.so.8\n",
      "2022-04-30 00:01:51.120363: I tensorflow/stream_executor/cuda/cuda_dnn.cc:359] Loaded cuDNN version 8101\n",
      "2022-04-30 00:01:52.246573: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcublas.so.11\n",
      "2022-04-30 00:01:53.044977: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcublasLt.so.11\n",
      "2022-04-30 00:01:53.069488: I tensorflow/stream_executor/cuda/cuda_blas.cc:1838] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 1/72 [..............................] - ETA: 5:41 - loss: 0.6931"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-30 00:01:53.578744: I tensorflow/core/profiler/lib/profiler_session.cc:126] Profiler session initializing.\n",
      "2022-04-30 00:01:53.578785: I tensorflow/core/profiler/lib/profiler_session.cc:141] Profiler session started.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 3/72 [>.............................] - ETA: 38s - loss: 0.6929 "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-30 00:01:54.508092: I tensorflow/core/profiler/lib/profiler_session.cc:66] Profiler session collecting data.\n",
      "2022-04-30 00:01:54.515641: I tensorflow/core/profiler/internal/gpu/cupti_tracer.cc:1743] CUPTI activity buffer flushed\n",
      "2022-04-30 00:01:54.596264: I tensorflow/core/profiler/internal/gpu/cupti_collector.cc:673]  GpuTracer has collected 317 callback api events and 315 activity events. \n",
      "2022-04-30 00:01:54.607430: I tensorflow/core/profiler/lib/profiler_session.cc:159] Profiler session tear down.\n",
      "2022-04-30 00:01:54.618235: I tensorflow/core/profiler/rpc/client/save_profile.cc:137] Creating directory: /tmp/autoencoder/train/plugins/profile/2022_04_30_00_01_54\n",
      "2022-04-30 00:01:54.629353: I tensorflow/core/profiler/rpc/client/save_profile.cc:143] Dumped gzipped tool data for trace.json.gz to /tmp/autoencoder/train/plugins/profile/2022_04_30_00_01_54/container-205611993c-92cd3b5f.trace.json.gz\n",
      "2022-04-30 00:01:54.648894: I tensorflow/core/profiler/rpc/client/save_profile.cc:137] Creating directory: /tmp/autoencoder/train/plugins/profile/2022_04_30_00_01_54\n",
      "2022-04-30 00:01:54.653358: I tensorflow/core/profiler/rpc/client/save_profile.cc:143] Dumped gzipped tool data for memory_profile.json.gz to /tmp/autoencoder/train/plugins/profile/2022_04_30_00_01_54/container-205611993c-92cd3b5f.memory_profile.json.gz\n",
      "2022-04-30 00:01:54.654137: I tensorflow/core/profiler/rpc/client/capture_profile.cc:251] Creating directory: /tmp/autoencoder/train/plugins/profile/2022_04_30_00_01_54Dumped tool data for xplane.pb to /tmp/autoencoder/train/plugins/profile/2022_04_30_00_01_54/container-205611993c-92cd3b5f.xplane.pb\n",
      "Dumped tool data for overview_page.pb to /tmp/autoencoder/train/plugins/profile/2022_04_30_00_01_54/container-205611993c-92cd3b5f.overview_page.pb\n",
      "Dumped tool data for input_pipeline.pb to /tmp/autoencoder/train/plugins/profile/2022_04_30_00_01_54/container-205611993c-92cd3b5f.input_pipeline.pb\n",
      "Dumped tool data for tensorflow_stats.pb to /tmp/autoencoder/train/plugins/profile/2022_04_30_00_01_54/container-205611993c-92cd3b5f.tensorflow_stats.pb\n",
      "Dumped tool data for kernel_stats.pb to /tmp/autoencoder/train/plugins/profile/2022_04_30_00_01_54/container-205611993c-92cd3b5f.kernel_stats.pb\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "72/72 [==============================] - 9s 53ms/step - loss: 0.6766 - val_loss: 0.6602\n",
      "Epoch 2/1000\n",
      "72/72 [==============================] - 2s 22ms/step - loss: 0.6512 - val_loss: 0.6475\n",
      "Epoch 3/1000\n",
      "72/72 [==============================] - 1s 20ms/step - loss: 0.6454 - val_loss: 0.6434\n",
      "Epoch 4/1000\n",
      "72/72 [==============================] - 1s 20ms/step - loss: 0.6426 - val_loss: 0.6413\n",
      "Epoch 5/1000\n",
      "72/72 [==============================] - 2s 22ms/step - loss: 0.6408 - val_loss: 0.6407\n",
      "Epoch 6/1000\n",
      "72/72 [==============================] - 1s 21ms/step - loss: 0.6401 - val_loss: 0.6397\n",
      "Epoch 7/1000\n",
      "72/72 [==============================] - 2s 24ms/step - loss: 0.6395 - val_loss: 0.6394\n",
      "Epoch 8/1000\n",
      "72/72 [==============================] - 1s 20ms/step - loss: 0.6392 - val_loss: 0.6390\n",
      "Epoch 9/1000\n",
      "72/72 [==============================] - 2s 22ms/step - loss: 0.6390 - val_loss: 0.6386\n",
      "Epoch 10/1000\n",
      "72/72 [==============================] - 2s 23ms/step - loss: 0.6388 - val_loss: 0.6388\n",
      "Epoch 11/1000\n",
      "72/72 [==============================] - 2s 33ms/step - loss: 0.6386 - val_loss: 0.6384\n",
      "Epoch 12/1000\n",
      "72/72 [==============================] - 2s 27ms/step - loss: 0.6385 - val_loss: 0.6382\n",
      "Epoch 13/1000\n",
      "72/72 [==============================] - 2s 25ms/step - loss: 0.6384 - val_loss: 0.6383\n",
      "Epoch 14/1000\n",
      "72/72 [==============================] - 2s 28ms/step - loss: 0.6383 - val_loss: 0.6386\n",
      "Epoch 15/1000\n",
      "72/72 [==============================] - 2s 28ms/step - loss: 0.6383 - val_loss: 0.6380\n",
      "Epoch 16/1000\n",
      "72/72 [==============================] - 2s 26ms/step - loss: 0.6382 - val_loss: 0.6383\n",
      "Epoch 17/1000\n",
      "72/72 [==============================] - 2s 24ms/step - loss: 0.6381 - val_loss: 0.6379\n",
      "Epoch 18/1000\n",
      "72/72 [==============================] - 2s 29ms/step - loss: 0.6382 - val_loss: 0.6379\n",
      "Epoch 19/1000\n",
      "72/72 [==============================] - 2s 30ms/step - loss: 0.6380 - val_loss: 0.6379\n",
      "Epoch 20/1000\n",
      "72/72 [==============================] - 2s 27ms/step - loss: 0.6379 - val_loss: 0.6378\n",
      "Epoch 21/1000\n",
      "72/72 [==============================] - 2s 28ms/step - loss: 0.6379 - val_loss: 0.6377\n",
      "Epoch 22/1000\n",
      "72/72 [==============================] - 2s 26ms/step - loss: 0.6379 - val_loss: 0.6378\n",
      "Epoch 23/1000\n",
      "72/72 [==============================] - 2s 24ms/step - loss: 0.6378 - val_loss: 0.6377\n",
      "Epoch 24/1000\n",
      "72/72 [==============================] - 2s 28ms/step - loss: 0.6377 - val_loss: 0.6377\n",
      "Epoch 25/1000\n",
      "72/72 [==============================] - 2s 31ms/step - loss: 0.6377 - val_loss: 0.6378\n",
      "Epoch 26/1000\n",
      "72/72 [==============================] - 2s 27ms/step - loss: 0.6379 - val_loss: 0.6382\n",
      "Epoch 27/1000\n",
      "72/72 [==============================] - 2s 23ms/step - loss: 0.6377 - val_loss: 0.6376\n",
      "Epoch 28/1000\n",
      "72/72 [==============================] - 2s 26ms/step - loss: 0.6377 - val_loss: 0.6376\n",
      "Epoch 29/1000\n",
      "72/72 [==============================] - 2s 26ms/step - loss: 0.6378 - val_loss: 0.6376\n",
      "Epoch 30/1000\n",
      "72/72 [==============================] - 2s 26ms/step - loss: 0.6376 - val_loss: 0.6375\n",
      "Epoch 31/1000\n",
      "72/72 [==============================] - 2s 25ms/step - loss: 0.6376 - val_loss: 0.6377\n",
      "Epoch 32/1000\n",
      "72/72 [==============================] - 2s 23ms/step - loss: 0.6376 - val_loss: 0.6375\n",
      "Epoch 33/1000\n",
      "72/72 [==============================] - 2s 23ms/step - loss: 0.6375 - val_loss: 0.6374\n",
      "Epoch 34/1000\n",
      "72/72 [==============================] - 2s 23ms/step - loss: 0.6375 - val_loss: 0.6376\n",
      "Epoch 35/1000\n",
      "72/72 [==============================] - 2s 23ms/step - loss: 0.6376 - val_loss: 0.6376\n",
      "Epoch 36/1000\n",
      "72/72 [==============================] - 2s 25ms/step - loss: 0.6375 - val_loss: 0.6377\n",
      "Epoch 37/1000\n",
      "72/72 [==============================] - 2s 24ms/step - loss: 0.6376 - val_loss: 0.6375\n",
      "Epoch 38/1000\n",
      "72/72 [==============================] - 2s 30ms/step - loss: 0.6375 - val_loss: 0.6374\n",
      "Epoch 39/1000\n",
      "72/72 [==============================] - 2s 28ms/step - loss: 0.6374 - val_loss: 0.6375\n",
      "Epoch 40/1000\n",
      "72/72 [==============================] - 2s 26ms/step - loss: 0.6376 - val_loss: 0.6375\n",
      "Epoch 41/1000\n",
      "72/72 [==============================] - 2s 26ms/step - loss: 0.6375 - val_loss: 0.6375\n",
      "Epoch 42/1000\n",
      "72/72 [==============================] - 2s 25ms/step - loss: 0.6374 - val_loss: 0.6375\n",
      "Epoch 43/1000\n",
      "72/72 [==============================] - 2s 25ms/step - loss: 0.6374 - val_loss: 0.6373\n",
      "Epoch 44/1000\n",
      "72/72 [==============================] - 2s 26ms/step - loss: 0.6373 - val_loss: 0.6374\n",
      "Epoch 45/1000\n",
      "72/72 [==============================] - 2s 27ms/step - loss: 0.6374 - val_loss: 0.6375\n",
      "Epoch 46/1000\n",
      "72/72 [==============================] - 2s 25ms/step - loss: 0.6374 - val_loss: 0.6372\n",
      "Epoch 47/1000\n",
      "72/72 [==============================] - 2s 23ms/step - loss: 0.6373 - val_loss: 0.6373\n",
      "Epoch 48/1000\n",
      "72/72 [==============================] - 2s 24ms/step - loss: 0.6374 - val_loss: 0.6381\n",
      "Epoch 49/1000\n",
      "72/72 [==============================] - 2s 24ms/step - loss: 0.6374 - val_loss: 0.6372\n",
      "Epoch 50/1000\n",
      "72/72 [==============================] - 2s 24ms/step - loss: 0.6373 - val_loss: 0.6373\n",
      "Epoch 51/1000\n",
      "72/72 [==============================] - 2s 23ms/step - loss: 0.6373 - val_loss: 0.6372\n",
      "Epoch 52/1000\n",
      "72/72 [==============================] - 2s 25ms/step - loss: 0.6373 - val_loss: 0.6372\n",
      "Epoch 53/1000\n",
      "72/72 [==============================] - 2s 24ms/step - loss: 0.6373 - val_loss: 0.6371\n",
      "Epoch 54/1000\n",
      "72/72 [==============================] - 2s 24ms/step - loss: 0.6372 - val_loss: 0.6373\n",
      "Epoch 55/1000\n",
      "72/72 [==============================] - 2s 23ms/step - loss: 0.6372 - val_loss: 0.6371\n",
      "Epoch 56/1000\n",
      "72/72 [==============================] - 2s 25ms/step - loss: 0.6372 - val_loss: 0.6371\n",
      "Epoch 57/1000\n",
      "72/72 [==============================] - 2s 30ms/step - loss: 0.6372 - val_loss: 0.6371\n",
      "Epoch 58/1000\n",
      "72/72 [==============================] - 2s 31ms/step - loss: 0.6372 - val_loss: 0.6369\n",
      "Epoch 59/1000\n",
      "72/72 [==============================] - 2s 25ms/step - loss: 0.6371 - val_loss: 0.6372\n",
      "Epoch 60/1000\n",
      "72/72 [==============================] - 2s 26ms/step - loss: 0.6371 - val_loss: 0.6369\n",
      "Epoch 61/1000\n",
      "72/72 [==============================] - 2s 24ms/step - loss: 0.6371 - val_loss: 0.6369\n",
      "Epoch 62/1000\n",
      "72/72 [==============================] - 2s 26ms/step - loss: 0.6370 - val_loss: 0.6372\n",
      "Epoch 63/1000\n",
      "72/72 [==============================] - 2s 25ms/step - loss: 0.6370 - val_loss: 0.6372\n",
      "Epoch 64/1000\n",
      "72/72 [==============================] - 2s 26ms/step - loss: 0.6372 - val_loss: 0.6371\n",
      "Epoch 65/1000\n",
      "72/72 [==============================] - 2s 24ms/step - loss: 0.6369 - val_loss: 0.6366\n",
      "Epoch 66/1000\n",
      "72/72 [==============================] - 2s 25ms/step - loss: 0.6369 - val_loss: 0.6372\n",
      "Epoch 67/1000\n",
      "72/72 [==============================] - 2s 24ms/step - loss: 0.6370 - val_loss: 0.6367\n",
      "Epoch 68/1000\n",
      "72/72 [==============================] - 2s 23ms/step - loss: 0.6370 - val_loss: 0.6366\n",
      "Epoch 69/1000\n",
      "72/72 [==============================] - 2s 23ms/step - loss: 0.6369 - val_loss: 0.6368\n",
      "Epoch 70/1000\n",
      "72/72 [==============================] - 2s 24ms/step - loss: 0.6369 - val_loss: 0.6369\n",
      "Epoch 71/1000\n",
      "72/72 [==============================] - 2s 24ms/step - loss: 0.6369 - val_loss: 0.6370\n",
      "Epoch 72/1000\n",
      "72/72 [==============================] - 2s 23ms/step - loss: 0.6368 - val_loss: 0.6373\n",
      "Epoch 73/1000\n",
      "72/72 [==============================] - 2s 24ms/step - loss: 0.6364 - val_loss: 0.6364\n",
      "Epoch 74/1000\n",
      "72/72 [==============================] - 2s 24ms/step - loss: 0.6367 - val_loss: 0.6363\n",
      "Epoch 75/1000\n",
      "72/72 [==============================] - 2s 24ms/step - loss: 0.6364 - val_loss: 0.6368\n",
      "Epoch 76/1000\n",
      "72/72 [==============================] - 2s 24ms/step - loss: 0.6363 - val_loss: 0.6362\n",
      "Epoch 77/1000\n",
      "72/72 [==============================] - 2s 24ms/step - loss: 0.6368 - val_loss: 0.6370\n",
      "Epoch 78/1000\n",
      "72/72 [==============================] - 2s 23ms/step - loss: 0.6366 - val_loss: 0.6362\n",
      "Epoch 79/1000\n",
      "72/72 [==============================] - 2s 26ms/step - loss: 0.6367 - val_loss: 0.6359\n",
      "Epoch 80/1000\n",
      "72/72 [==============================] - 2s 31ms/step - loss: 0.6365 - val_loss: 0.6365\n",
      "Epoch 81/1000\n",
      "72/72 [==============================] - 2s 28ms/step - loss: 0.6364 - val_loss: 0.6369\n",
      "Epoch 82/1000\n",
      "72/72 [==============================] - 2s 27ms/step - loss: 0.6366 - val_loss: 0.6365\n",
      "Epoch 83/1000\n",
      "72/72 [==============================] - 2s 25ms/step - loss: 0.6362 - val_loss: 0.6360\n",
      "Epoch 84/1000\n",
      "72/72 [==============================] - 2s 25ms/step - loss: 0.6362 - val_loss: 0.6362\n",
      "Epoch 85/1000\n",
      "72/72 [==============================] - 2s 26ms/step - loss: 0.6360 - val_loss: 0.6361\n",
      "Epoch 86/1000\n",
      "72/72 [==============================] - 2s 25ms/step - loss: 0.6364 - val_loss: 0.6373\n",
      "Epoch 87/1000\n",
      "72/72 [==============================] - 2s 27ms/step - loss: 0.6361 - val_loss: 0.6359\n",
      "Epoch 88/1000\n",
      "72/72 [==============================] - 2s 25ms/step - loss: 0.6361 - val_loss: 0.6363\n",
      "Epoch 89/1000\n",
      "72/72 [==============================] - 2s 23ms/step - loss: 0.6366 - val_loss: 0.6356\n",
      "Epoch 90/1000\n",
      "72/72 [==============================] - 2s 24ms/step - loss: 0.6361 - val_loss: 0.6363\n",
      "Epoch 91/1000\n",
      "72/72 [==============================] - 2s 22ms/step - loss: 0.6361 - val_loss: 0.6377\n",
      "Epoch 92/1000\n",
      "72/72 [==============================] - 2s 23ms/step - loss: 0.6363 - val_loss: 0.6353\n",
      "Epoch 93/1000\n",
      "72/72 [==============================] - 2s 24ms/step - loss: 0.6360 - val_loss: 0.6360\n",
      "Epoch 94/1000\n",
      "72/72 [==============================] - 2s 23ms/step - loss: 0.6365 - val_loss: 0.6356\n",
      "Epoch 95/1000\n",
      "72/72 [==============================] - 2s 22ms/step - loss: 0.6360 - val_loss: 0.6360\n",
      "Epoch 96/1000\n",
      "72/72 [==============================] - 2s 24ms/step - loss: 0.6358 - val_loss: 0.6366\n",
      "Epoch 97/1000\n",
      "72/72 [==============================] - 2s 24ms/step - loss: 0.6356 - val_loss: 0.6355\n",
      "Epoch 98/1000\n",
      "72/72 [==============================] - 2s 23ms/step - loss: 0.6357 - val_loss: 0.6364\n",
      "Epoch 99/1000\n",
      "72/72 [==============================] - 2s 22ms/step - loss: 0.6360 - val_loss: 0.6356\n",
      "Epoch 100/1000\n",
      "72/72 [==============================] - 2s 23ms/step - loss: 0.6354 - val_loss: 0.6353\n",
      "Epoch 101/1000\n",
      "72/72 [==============================] - 2s 23ms/step - loss: 0.6353 - val_loss: 0.6348\n",
      "Epoch 102/1000\n",
      "72/72 [==============================] - 2s 24ms/step - loss: 0.6354 - val_loss: 0.6350\n",
      "Epoch 103/1000\n",
      "72/72 [==============================] - 2s 25ms/step - loss: 0.6354 - val_loss: 0.6371\n",
      "Epoch 104/1000\n",
      "72/72 [==============================] - 2s 28ms/step - loss: 0.6371 - val_loss: 0.6358\n",
      "Epoch 105/1000\n",
      "72/72 [==============================] - 2s 27ms/step - loss: 0.6362 - val_loss: 0.6357\n",
      "Epoch 106/1000\n",
      "72/72 [==============================] - 2s 26ms/step - loss: 0.6354 - val_loss: 0.6352\n",
      "Epoch 107/1000\n",
      "72/72 [==============================] - 2s 26ms/step - loss: 0.6355 - val_loss: 0.6353\n",
      "Epoch 108/1000\n",
      "72/72 [==============================] - 2s 24ms/step - loss: 0.6354 - val_loss: 0.6349\n",
      "Epoch 109/1000\n",
      "72/72 [==============================] - 2s 25ms/step - loss: 0.6361 - val_loss: 0.6379\n",
      "Epoch 110/1000\n",
      "72/72 [==============================] - 2s 24ms/step - loss: 0.6358 - val_loss: 0.6361\n",
      "Epoch 111/1000\n",
      "72/72 [==============================] - 2s 26ms/step - loss: 0.6355 - val_loss: 0.6350\n",
      "Epoch 112/1000\n",
      "72/72 [==============================] - 2s 24ms/step - loss: 0.6354 - val_loss: 0.6355\n",
      "Epoch 113/1000\n",
      "72/72 [==============================] - 2s 23ms/step - loss: 0.6351 - val_loss: 0.6351\n",
      "Epoch 114/1000\n",
      "72/72 [==============================] - 2s 23ms/step - loss: 0.6353 - val_loss: 0.6349\n",
      "Epoch 115/1000\n",
      "72/72 [==============================] - 2s 24ms/step - loss: 0.6359 - val_loss: 0.6349\n",
      "Epoch 116/1000\n",
      "72/72 [==============================] - 2s 24ms/step - loss: 0.6355 - val_loss: 0.6352\n",
      "Epoch 117/1000\n",
      "72/72 [==============================] - 2s 22ms/step - loss: 0.6353 - val_loss: 0.6370\n",
      "Epoch 118/1000\n",
      "72/72 [==============================] - 2s 24ms/step - loss: 0.6387 - val_loss: 0.6378\n",
      "Epoch 119/1000\n",
      "72/72 [==============================] - 2s 24ms/step - loss: 0.6380 - val_loss: 0.6379\n",
      "Epoch 120/1000\n",
      "72/72 [==============================] - 2s 24ms/step - loss: 0.6378 - val_loss: 0.6379\n",
      "Epoch 121/1000\n",
      "72/72 [==============================] - 2s 23ms/step - loss: 0.6377 - val_loss: 0.6376\n",
      "Epoch 122/1000\n",
      "72/72 [==============================] - 2s 24ms/step - loss: 0.6375 - val_loss: 0.6374\n",
      "Epoch 123/1000\n",
      "72/72 [==============================] - 2s 25ms/step - loss: 0.6374 - val_loss: 0.6372\n",
      "Epoch 124/1000\n",
      "72/72 [==============================] - 2s 25ms/step - loss: 0.6373 - val_loss: 0.6371\n",
      "Epoch 125/1000\n",
      "72/72 [==============================] - 2s 23ms/step - loss: 0.6372 - val_loss: 0.6372\n",
      "Epoch 126/1000\n",
      "72/72 [==============================] - 2s 23ms/step - loss: 0.6371 - val_loss: 0.6373\n",
      "Epoch 127/1000\n",
      "72/72 [==============================] - 2s 24ms/step - loss: 0.6367 - val_loss: 0.6359\n",
      "Epoch 128/1000\n",
      "72/72 [==============================] - 2s 24ms/step - loss: 0.6368 - val_loss: 0.6379\n",
      "Epoch 129/1000\n",
      "72/72 [==============================] - 2s 24ms/step - loss: 0.6375 - val_loss: 0.6373\n",
      "Epoch 130/1000\n",
      "72/72 [==============================] - 2s 25ms/step - loss: 0.6372 - val_loss: 0.6372\n",
      "Epoch 131/1000\n",
      "72/72 [==============================] - 2s 28ms/step - loss: 0.6372 - val_loss: 0.6369\n",
      "Epoch 132/1000\n",
      "72/72 [==============================] - 2s 25ms/step - loss: 0.6371 - val_loss: 0.6369\n",
      "Epoch 133/1000\n",
      "72/72 [==============================] - 2s 27ms/step - loss: 0.6372 - val_loss: 0.6367\n",
      "Epoch 134/1000\n",
      "72/72 [==============================] - 2s 26ms/step - loss: 0.6368 - val_loss: 0.6370\n",
      "Epoch 135/1000\n",
      "72/72 [==============================] - 2s 24ms/step - loss: 0.6367 - val_loss: 0.6388\n",
      "Epoch 136/1000\n",
      "72/72 [==============================] - 2s 25ms/step - loss: 0.6374 - val_loss: 0.6363\n",
      "Epoch 137/1000\n",
      "72/72 [==============================] - 2s 24ms/step - loss: 0.6363 - val_loss: 0.6353\n",
      "Epoch 138/1000\n",
      "72/72 [==============================] - 2s 26ms/step - loss: 0.6368 - val_loss: 0.6368\n",
      "Epoch 139/1000\n",
      "72/72 [==============================] - 2s 24ms/step - loss: 0.6368 - val_loss: 0.6365\n",
      "Epoch 140/1000\n",
      "72/72 [==============================] - 2s 25ms/step - loss: 0.6366 - val_loss: 0.6366\n",
      "Epoch 141/1000\n",
      "72/72 [==============================] - 2s 25ms/step - loss: 0.6365 - val_loss: 0.6373\n",
      "Epoch 142/1000\n",
      "72/72 [==============================] - 2s 23ms/step - loss: 0.6367 - val_loss: 0.6366\n",
      "Epoch 143/1000\n",
      "72/72 [==============================] - 2s 24ms/step - loss: 0.6367 - val_loss: 0.6368\n",
      "Epoch 144/1000\n",
      "72/72 [==============================] - 2s 22ms/step - loss: 0.6364 - val_loss: 0.6360\n",
      "Epoch 145/1000\n",
      "72/72 [==============================] - 2s 23ms/step - loss: 0.6367 - val_loss: 0.6370\n",
      "Epoch 146/1000\n",
      "72/72 [==============================] - 2s 23ms/step - loss: 0.6364 - val_loss: 0.6358\n",
      "Epoch 147/1000\n",
      "72/72 [==============================] - 2s 25ms/step - loss: 0.6363 - val_loss: 0.6359\n",
      "Epoch 148/1000\n",
      "72/72 [==============================] - 2s 24ms/step - loss: 0.6362 - val_loss: 0.6373\n",
      "Epoch 149/1000\n",
      "72/72 [==============================] - 2s 22ms/step - loss: 0.6367 - val_loss: 0.6370\n",
      "Epoch 150/1000\n",
      "72/72 [==============================] - 2s 23ms/step - loss: 0.6363 - val_loss: 0.6362\n",
      "Epoch 151/1000\n",
      "72/72 [==============================] - 2s 23ms/step - loss: 0.6360 - val_loss: 0.6359\n",
      "Epoch 152/1000\n",
      "72/72 [==============================] - 2s 23ms/step - loss: 0.6358 - val_loss: 0.6354\n",
      "Epoch 153/1000\n",
      "72/72 [==============================] - 2s 23ms/step - loss: 0.6359 - val_loss: 0.6360\n",
      "Epoch 154/1000\n",
      "72/72 [==============================] - 2s 24ms/step - loss: 0.6352 - val_loss: 0.6355\n",
      "Epoch 155/1000\n",
      "72/72 [==============================] - 2s 23ms/step - loss: 0.6359 - val_loss: 0.6363\n",
      "Epoch 156/1000\n",
      "72/72 [==============================] - 2s 23ms/step - loss: 0.6358 - val_loss: 0.6351\n",
      "Epoch 157/1000\n",
      "72/72 [==============================] - 2s 23ms/step - loss: 0.6355 - val_loss: 0.6356\n",
      "Epoch 158/1000\n",
      "72/72 [==============================] - 2s 28ms/step - loss: 0.6351 - val_loss: 0.6363\n",
      "Epoch 159/1000\n",
      "72/72 [==============================] - 2s 30ms/step - loss: 0.6373 - val_loss: 0.6368\n",
      "Epoch 160/1000\n",
      "72/72 [==============================] - 2s 28ms/step - loss: 0.6364 - val_loss: 0.6368\n",
      "Epoch 161/1000\n",
      "72/72 [==============================] - 2s 25ms/step - loss: 0.6355 - val_loss: 0.6356\n",
      "Epoch 162/1000\n",
      "72/72 [==============================] - 2s 24ms/step - loss: 0.6354 - val_loss: 0.6350\n",
      "Epoch 163/1000\n",
      "72/72 [==============================] - 2s 25ms/step - loss: 0.6355 - val_loss: 0.6361\n",
      "Epoch 164/1000\n",
      "72/72 [==============================] - 2s 23ms/step - loss: 0.6364 - val_loss: 0.6365\n",
      "Epoch 165/1000\n",
      "72/72 [==============================] - 2s 26ms/step - loss: 0.6354 - val_loss: 0.6350\n",
      "Epoch 166/1000\n",
      "72/72 [==============================] - 2s 25ms/step - loss: 0.6350 - val_loss: 0.6346\n",
      "Epoch 167/1000\n",
      "72/72 [==============================] - 2s 25ms/step - loss: 0.6351 - val_loss: 0.6354\n",
      "Epoch 168/1000\n",
      "72/72 [==============================] - 2s 26ms/step - loss: 0.6353 - val_loss: 0.6349\n",
      "Epoch 169/1000\n",
      "72/72 [==============================] - 2s 24ms/step - loss: 0.6348 - val_loss: 0.6350\n",
      "Epoch 170/1000\n",
      "72/72 [==============================] - 2s 23ms/step - loss: 0.6351 - val_loss: 0.6344\n",
      "Epoch 171/1000\n",
      "72/72 [==============================] - 2s 23ms/step - loss: 0.6347 - val_loss: 0.6347\n",
      "Epoch 172/1000\n",
      "72/72 [==============================] - 2s 24ms/step - loss: 0.6382 - val_loss: 0.6362\n",
      "Epoch 173/1000\n",
      "72/72 [==============================] - 2s 24ms/step - loss: 0.6350 - val_loss: 0.6345\n",
      "Epoch 174/1000\n",
      "72/72 [==============================] - 2s 24ms/step - loss: 0.6347 - val_loss: 0.6342\n",
      "Epoch 175/1000\n",
      "72/72 [==============================] - 2s 23ms/step - loss: 0.6348 - val_loss: 0.6348\n",
      "Epoch 176/1000\n",
      "72/72 [==============================] - 2s 23ms/step - loss: 0.6361 - val_loss: 0.6371\n",
      "Epoch 177/1000\n",
      "72/72 [==============================] - 2s 24ms/step - loss: 0.6369 - val_loss: 0.6380\n",
      "Epoch 178/1000\n",
      "72/72 [==============================] - 2s 24ms/step - loss: 0.6358 - val_loss: 0.6357\n",
      "Epoch 179/1000\n",
      "72/72 [==============================] - 2s 24ms/step - loss: 0.6351 - val_loss: 0.6388\n",
      "Epoch 180/1000\n",
      "72/72 [==============================] - 2s 24ms/step - loss: 0.6379 - val_loss: 0.6370\n",
      "Epoch 181/1000\n",
      "72/72 [==============================] - 2s 23ms/step - loss: 0.6370 - val_loss: 0.6375\n",
      "Epoch 182/1000\n",
      "72/72 [==============================] - 2s 23ms/step - loss: 0.6362 - val_loss: 0.6348\n",
      "Epoch 183/1000\n",
      "72/72 [==============================] - 2s 24ms/step - loss: 0.6359 - val_loss: 0.6350\n",
      "Epoch 184/1000\n",
      "72/72 [==============================] - 2s 24ms/step - loss: 0.6355 - val_loss: 0.6361\n",
      "Epoch 185/1000\n",
      "72/72 [==============================] - 2s 23ms/step - loss: 0.6354 - val_loss: 0.6354\n",
      "Epoch 186/1000\n",
      "72/72 [==============================] - 2s 23ms/step - loss: 0.6358 - val_loss: 0.6365\n",
      "Epoch 187/1000\n",
      "72/72 [==============================] - 2s 27ms/step - loss: 0.6355 - val_loss: 0.6345\n",
      "Epoch 188/1000\n",
      "72/72 [==============================] - 2s 29ms/step - loss: 0.6347 - val_loss: 0.6345\n",
      "Epoch 189/1000\n",
      "72/72 [==============================] - 2s 28ms/step - loss: 0.6365 - val_loss: 0.6362\n",
      "Epoch 190/1000\n",
      "72/72 [==============================] - 2s 28ms/step - loss: 0.6356 - val_loss: 0.6349\n",
      "Epoch 191/1000\n",
      "72/72 [==============================] - 2s 27ms/step - loss: 0.6349 - val_loss: 0.6354\n",
      "Epoch 192/1000\n",
      "72/72 [==============================] - 2s 26ms/step - loss: 0.6351 - val_loss: 0.6347\n",
      "Epoch 193/1000\n",
      "72/72 [==============================] - 2s 23ms/step - loss: 0.6349 - val_loss: 0.6348\n",
      "Epoch 194/1000\n",
      "72/72 [==============================] - 2s 23ms/step - loss: 0.6349 - val_loss: 0.6367\n",
      "Epoch 195/1000\n",
      "72/72 [==============================] - 2s 24ms/step - loss: 0.6364 - val_loss: 0.6357\n",
      "Epoch 196/1000\n",
      "72/72 [==============================] - 2s 24ms/step - loss: 0.6349 - val_loss: 0.6349\n",
      "Epoch 197/1000\n",
      "72/72 [==============================] - 2s 24ms/step - loss: 0.6347 - val_loss: 0.6350\n",
      "Epoch 198/1000\n",
      "72/72 [==============================] - 2s 24ms/step - loss: 0.6353 - val_loss: 0.6347\n",
      "Epoch 199/1000\n",
      "72/72 [==============================] - 2s 24ms/step - loss: 0.6359 - val_loss: 0.6369\n",
      "Epoch 200/1000\n",
      "72/72 [==============================] - 2s 24ms/step - loss: 0.6360 - val_loss: 0.6357\n",
      "Epoch 201/1000\n",
      "72/72 [==============================] - 2s 25ms/step - loss: 0.6350 - val_loss: 0.6349\n",
      "Epoch 202/1000\n",
      "72/72 [==============================] - 2s 23ms/step - loss: 0.6347 - val_loss: 0.6347\n",
      "Epoch 203/1000\n",
      "72/72 [==============================] - 2s 24ms/step - loss: 0.6348 - val_loss: 0.6343\n",
      "Epoch 204/1000\n",
      "72/72 [==============================] - 2s 24ms/step - loss: 0.6344 - val_loss: 0.6349\n",
      "Epoch 205/1000\n",
      "72/72 [==============================] - 2s 22ms/step - loss: 0.6362 - val_loss: 0.6370\n",
      "Epoch 206/1000\n",
      "72/72 [==============================] - 2s 23ms/step - loss: 0.6371 - val_loss: 0.6365\n",
      "Epoch 207/1000\n",
      "72/72 [==============================] - 2s 23ms/step - loss: 0.6363 - val_loss: 0.6365\n",
      "Epoch 208/1000\n",
      "72/72 [==============================] - 2s 24ms/step - loss: 0.6375 - val_loss: 0.6378\n",
      "Epoch 209/1000\n",
      "72/72 [==============================] - 2s 24ms/step - loss: 0.6368 - val_loss: 0.6355\n",
      "Epoch 210/1000\n",
      "72/72 [==============================] - 2s 26ms/step - loss: 0.6352 - val_loss: 0.6345\n",
      "Epoch 211/1000\n",
      "72/72 [==============================] - 2s 24ms/step - loss: 0.6344 - val_loss: 0.6343\n",
      "Epoch 212/1000\n",
      "72/72 [==============================] - 2s 24ms/step - loss: 0.6341 - val_loss: 0.6344\n",
      "Epoch 213/1000\n",
      "72/72 [==============================] - 2s 25ms/step - loss: 0.6345 - val_loss: 0.6350\n",
      "Epoch 214/1000\n",
      "72/72 [==============================] - 2s 23ms/step - loss: 0.6355 - val_loss: 0.6348\n",
      "Epoch 215/1000\n",
      "72/72 [==============================] - 2s 24ms/step - loss: 0.6346 - val_loss: 0.6339\n",
      "Epoch 216/1000\n",
      "72/72 [==============================] - 2s 24ms/step - loss: 0.6349 - val_loss: 0.6360\n",
      "Epoch 217/1000\n",
      "72/72 [==============================] - 2s 24ms/step - loss: 0.6344 - val_loss: 0.6345\n",
      "Epoch 218/1000\n",
      "72/72 [==============================] - 2s 24ms/step - loss: 0.6349 - val_loss: 0.6348\n",
      "Epoch 219/1000\n",
      "72/72 [==============================] - 2s 25ms/step - loss: 0.6349 - val_loss: 0.6343\n",
      "Epoch 220/1000\n",
      "72/72 [==============================] - 2s 24ms/step - loss: 0.6345 - val_loss: 0.6350\n",
      "Epoch 221/1000\n",
      "72/72 [==============================] - 2s 24ms/step - loss: 0.6355 - val_loss: 0.6346\n",
      "Epoch 222/1000\n",
      "72/72 [==============================] - 2s 23ms/step - loss: 0.6363 - val_loss: 0.6401\n",
      "Epoch 223/1000\n",
      "72/72 [==============================] - 2s 24ms/step - loss: 0.6374 - val_loss: 0.6351\n",
      "Epoch 224/1000\n",
      "72/72 [==============================] - 2s 24ms/step - loss: 0.6345 - val_loss: 0.6345\n",
      "Epoch 225/1000\n",
      "72/72 [==============================] - 2s 24ms/step - loss: 0.6339 - val_loss: 0.6335\n",
      "Epoch 226/1000\n",
      "72/72 [==============================] - 2s 24ms/step - loss: 0.6340 - val_loss: 0.6338\n",
      "Epoch 227/1000\n",
      "72/72 [==============================] - 2s 24ms/step - loss: 0.6341 - val_loss: 0.6340\n",
      "Epoch 228/1000\n",
      "72/72 [==============================] - 2s 25ms/step - loss: 0.6351 - val_loss: 0.6348\n",
      "Epoch 229/1000\n",
      "72/72 [==============================] - 2s 24ms/step - loss: 0.6347 - val_loss: 0.6355\n",
      "Epoch 230/1000\n",
      "72/72 [==============================] - 2s 23ms/step - loss: 0.6354 - val_loss: 0.6354\n",
      "Epoch 231/1000\n",
      "72/72 [==============================] - 2s 23ms/step - loss: 0.6349 - val_loss: 0.6358\n",
      "Epoch 232/1000\n",
      "72/72 [==============================] - 2s 24ms/step - loss: 0.6374 - val_loss: 0.6374\n",
      "Epoch 233/1000\n",
      "72/72 [==============================] - 2s 24ms/step - loss: 0.6373 - val_loss: 0.6365\n",
      "Epoch 234/1000\n",
      "72/72 [==============================] - 2s 23ms/step - loss: 0.6360 - val_loss: 0.6353\n",
      "Epoch 235/1000\n",
      "72/72 [==============================] - 2s 24ms/step - loss: 0.6360 - val_loss: 0.6357\n",
      "Epoch 236/1000\n",
      "72/72 [==============================] - 2s 24ms/step - loss: 0.6358 - val_loss: 0.6356\n",
      "Epoch 237/1000\n",
      "72/72 [==============================] - 2s 23ms/step - loss: 0.6354 - val_loss: 0.6351\n",
      "Epoch 238/1000\n",
      "72/72 [==============================] - 2s 24ms/step - loss: 0.6353 - val_loss: 0.6349\n",
      "Epoch 239/1000\n",
      "72/72 [==============================] - 2s 24ms/step - loss: 0.6351 - val_loss: 0.6350\n",
      "Epoch 240/1000\n",
      "72/72 [==============================] - 2s 23ms/step - loss: 0.6356 - val_loss: 0.6358\n",
      "Epoch 241/1000\n",
      "72/72 [==============================] - 2s 23ms/step - loss: 0.6354 - val_loss: 0.6348\n",
      "Epoch 242/1000\n",
      "72/72 [==============================] - 2s 23ms/step - loss: 0.6344 - val_loss: 0.6338\n",
      "Epoch 243/1000\n",
      "72/72 [==============================] - 2s 24ms/step - loss: 0.6340 - val_loss: 0.6341\n",
      "Epoch 244/1000\n",
      "72/72 [==============================] - 2s 23ms/step - loss: 0.6342 - val_loss: 0.6367\n",
      "Epoch 245/1000\n",
      "72/72 [==============================] - 2s 24ms/step - loss: 0.6351 - val_loss: 0.6344\n",
      "Epoch 246/1000\n",
      "72/72 [==============================] - 2s 26ms/step - loss: 0.6376 - val_loss: 0.6384\n",
      "Epoch 247/1000\n",
      "72/72 [==============================] - 2s 27ms/step - loss: 0.6378 - val_loss: 0.6373\n",
      "Epoch 248/1000\n",
      "72/72 [==============================] - 2s 27ms/step - loss: 0.6373 - val_loss: 0.6371\n",
      "Epoch 249/1000\n",
      "72/72 [==============================] - 2s 25ms/step - loss: 0.6369 - val_loss: 0.6368\n",
      "Epoch 250/1000\n",
      "72/72 [==============================] - 2s 25ms/step - loss: 0.6357 - val_loss: 0.6352\n",
      "Epoch 251/1000\n",
      "72/72 [==============================] - 2s 24ms/step - loss: 0.6363 - val_loss: 0.6410\n",
      "Epoch 252/1000\n",
      "72/72 [==============================] - 2s 25ms/step - loss: 0.6358 - val_loss: 0.6350\n",
      "Epoch 253/1000\n",
      "72/72 [==============================] - 2s 23ms/step - loss: 0.6366 - val_loss: 0.6350\n",
      "Epoch 254/1000\n",
      "72/72 [==============================] - 2s 25ms/step - loss: 0.6343 - val_loss: 0.6336\n",
      "Epoch 255/1000\n",
      "72/72 [==============================] - 2s 25ms/step - loss: 0.6336 - val_loss: 0.6334\n",
      "Epoch 256/1000\n",
      "72/72 [==============================] - 2s 25ms/step - loss: 0.6339 - val_loss: 0.6347\n",
      "Epoch 257/1000\n",
      "72/72 [==============================] - 2s 24ms/step - loss: 0.6343 - val_loss: 0.6340\n",
      "Epoch 258/1000\n",
      "72/72 [==============================] - 2s 24ms/step - loss: 0.6351 - val_loss: 0.6356\n",
      "Epoch 259/1000\n",
      "72/72 [==============================] - 2s 23ms/step - loss: 0.6355 - val_loss: 0.6354\n",
      "Epoch 260/1000\n",
      "72/72 [==============================] - 2s 24ms/step - loss: 0.6353 - val_loss: 0.6351\n",
      "Epoch 261/1000\n",
      "72/72 [==============================] - 2s 24ms/step - loss: 0.6349 - val_loss: 0.6344\n",
      "Epoch 262/1000\n",
      "72/72 [==============================] - 2s 23ms/step - loss: 0.6346 - val_loss: 0.6351\n",
      "Epoch 263/1000\n",
      "72/72 [==============================] - 2s 23ms/step - loss: 0.6350 - val_loss: 0.6345\n",
      "Epoch 264/1000\n",
      "72/72 [==============================] - 2s 23ms/step - loss: 0.6344 - val_loss: 0.6342\n",
      "Epoch 265/1000\n",
      "72/72 [==============================] - 2s 23ms/step - loss: 0.6344 - val_loss: 0.6339\n",
      "Epoch 266/1000\n",
      "72/72 [==============================] - 2s 24ms/step - loss: 0.6341 - val_loss: 0.6338\n",
      "Epoch 267/1000\n",
      "72/72 [==============================] - 2s 23ms/step - loss: 0.6342 - val_loss: 0.6341\n",
      "Epoch 268/1000\n",
      "72/72 [==============================] - 2s 23ms/step - loss: 0.6353 - val_loss: 0.6348\n",
      "Epoch 269/1000\n",
      "72/72 [==============================] - 2s 23ms/step - loss: 0.6347 - val_loss: 0.6349\n",
      "Epoch 270/1000\n",
      "72/72 [==============================] - 2s 23ms/step - loss: 0.6361 - val_loss: 0.6346\n",
      "Epoch 271/1000\n",
      "72/72 [==============================] - 2s 23ms/step - loss: 0.6343 - val_loss: 0.6345\n",
      "Epoch 272/1000\n",
      "72/72 [==============================] - 2s 24ms/step - loss: 0.6345 - val_loss: 0.6343\n",
      "Epoch 273/1000\n",
      "72/72 [==============================] - 2s 24ms/step - loss: 0.6341 - val_loss: 0.6337\n",
      "Epoch 274/1000\n",
      "72/72 [==============================] - 2s 23ms/step - loss: 0.6351 - val_loss: 0.6379\n",
      "Epoch 275/1000\n",
      "72/72 [==============================] - 2s 23ms/step - loss: 0.6369 - val_loss: 0.6441\n",
      "Epoch 276/1000\n",
      "72/72 [==============================] - 2s 23ms/step - loss: 0.6403 - val_loss: 0.6396\n",
      "Epoch 277/1000\n",
      "72/72 [==============================] - 2s 24ms/step - loss: 0.6365 - val_loss: 0.6351\n",
      "Epoch 278/1000\n",
      "72/72 [==============================] - 2s 26ms/step - loss: 0.6352 - val_loss: 0.6351\n",
      "Epoch 279/1000\n",
      "72/72 [==============================] - 2s 29ms/step - loss: 0.6358 - val_loss: 0.6352\n",
      "Epoch 280/1000\n",
      "72/72 [==============================] - 2s 29ms/step - loss: 0.6346 - val_loss: 0.6338\n",
      "Epoch 281/1000\n",
      "72/72 [==============================] - 2s 27ms/step - loss: 0.6341 - val_loss: 0.6349\n",
      "Epoch 282/1000\n",
      "72/72 [==============================] - 2s 24ms/step - loss: 0.6362 - val_loss: 0.6371\n",
      "Epoch 283/1000\n",
      "72/72 [==============================] - 2s 25ms/step - loss: 0.6352 - val_loss: 0.6343\n",
      "Epoch 284/1000\n",
      "72/72 [==============================] - 2s 24ms/step - loss: 0.6339 - val_loss: 0.6338\n",
      "Epoch 285/1000\n",
      "72/72 [==============================] - 2s 24ms/step - loss: 0.6335 - val_loss: 0.6334\n",
      "Epoch 286/1000\n",
      "72/72 [==============================] - 2s 26ms/step - loss: 0.6342 - val_loss: 0.6342\n",
      "Epoch 287/1000\n",
      "72/72 [==============================] - 2s 25ms/step - loss: 0.6335 - val_loss: 0.6336\n",
      "Epoch 288/1000\n",
      "72/72 [==============================] - 2s 26ms/step - loss: 0.6334 - val_loss: 0.6333\n",
      "Epoch 289/1000\n",
      "72/72 [==============================] - 2s 26ms/step - loss: 0.6336 - val_loss: 0.6342\n",
      "Epoch 290/1000\n",
      "72/72 [==============================] - 2s 24ms/step - loss: 0.6345 - val_loss: 0.6337\n",
      "Epoch 291/1000\n",
      "72/72 [==============================] - 2s 24ms/step - loss: 0.6350 - val_loss: 0.6399\n",
      "Epoch 292/1000\n",
      "72/72 [==============================] - 2s 23ms/step - loss: 0.6360 - val_loss: 0.6334\n",
      "Epoch 293/1000\n",
      "72/72 [==============================] - 2s 22ms/step - loss: 0.6335 - val_loss: 0.6333\n",
      "Epoch 294/1000\n",
      "72/72 [==============================] - 2s 23ms/step - loss: 0.6337 - val_loss: 0.6337\n",
      "Epoch 295/1000\n",
      "72/72 [==============================] - 2s 23ms/step - loss: 0.6341 - val_loss: 0.6337\n",
      "Epoch 296/1000\n",
      "72/72 [==============================] - 2s 23ms/step - loss: 0.6335 - val_loss: 0.6335\n",
      "Epoch 297/1000\n",
      "72/72 [==============================] - 2s 25ms/step - loss: 0.6342 - val_loss: 0.6347\n",
      "Epoch 298/1000\n",
      "72/72 [==============================] - 2s 25ms/step - loss: 0.6354 - val_loss: 0.6351\n",
      "Epoch 299/1000\n",
      "72/72 [==============================] - 2s 23ms/step - loss: 0.6341 - val_loss: 0.6337\n",
      "Epoch 300/1000\n",
      "72/72 [==============================] - 2s 24ms/step - loss: 0.6338 - val_loss: 0.6351\n",
      "Epoch 301/1000\n",
      "72/72 [==============================] - 2s 23ms/step - loss: 0.6340 - val_loss: 0.6325\n",
      "Epoch 302/1000\n",
      "72/72 [==============================] - 2s 24ms/step - loss: 0.6332 - val_loss: 0.6326\n",
      "Epoch 303/1000\n",
      "72/72 [==============================] - 2s 23ms/step - loss: 0.6330 - val_loss: 0.6327\n",
      "Epoch 304/1000\n",
      "72/72 [==============================] - 2s 22ms/step - loss: 0.6332 - val_loss: 0.6342\n",
      "Epoch 305/1000\n",
      "72/72 [==============================] - 2s 23ms/step - loss: 0.6347 - val_loss: 0.6363\n",
      "Epoch 306/1000\n",
      "72/72 [==============================] - 2s 23ms/step - loss: 0.6390 - val_loss: 0.6393\n",
      "Epoch 307/1000\n",
      "72/72 [==============================] - 2s 23ms/step - loss: 0.6388 - val_loss: 0.6391\n",
      "Epoch 308/1000\n",
      "72/72 [==============================] - 2s 22ms/step - loss: 0.6389 - val_loss: 0.6384\n",
      "Epoch 309/1000\n",
      "72/72 [==============================] - 2s 23ms/step - loss: 0.6385 - val_loss: 0.6382\n",
      "Epoch 310/1000\n",
      "72/72 [==============================] - 2s 24ms/step - loss: 0.6382 - val_loss: 0.6380\n",
      "Epoch 311/1000\n",
      "72/72 [==============================] - 2s 25ms/step - loss: 0.6381 - val_loss: 0.6381\n",
      "Epoch 312/1000\n",
      "72/72 [==============================] - 2s 26ms/step - loss: 0.6381 - val_loss: 0.6380\n",
      "Epoch 313/1000\n",
      "72/72 [==============================] - 2s 26ms/step - loss: 0.6380 - val_loss: 0.6378\n",
      "Epoch 314/1000\n",
      "72/72 [==============================] - 2s 25ms/step - loss: 0.6380 - val_loss: 0.6377\n",
      "Epoch 315/1000\n",
      "72/72 [==============================] - 2s 24ms/step - loss: 0.6378 - val_loss: 0.6378\n",
      "Epoch 316/1000\n",
      "72/72 [==============================] - 2s 25ms/step - loss: 0.6378 - val_loss: 0.6378\n",
      "Epoch 317/1000\n",
      "72/72 [==============================] - 2s 25ms/step - loss: 0.6379 - val_loss: 0.6378\n",
      "Epoch 318/1000\n",
      "72/72 [==============================] - 2s 24ms/step - loss: 0.6378 - val_loss: 0.6377\n",
      "Epoch 319/1000\n",
      "72/72 [==============================] - 2s 25ms/step - loss: 0.6377 - val_loss: 0.6377\n",
      "Epoch 320/1000\n",
      "72/72 [==============================] - 2s 26ms/step - loss: 0.6377 - val_loss: 0.6380\n",
      "Epoch 321/1000\n",
      "72/72 [==============================] - 2s 25ms/step - loss: 0.6377 - val_loss: 0.6378\n",
      "Epoch 322/1000\n",
      "72/72 [==============================] - 2s 24ms/step - loss: 0.6375 - val_loss: 0.6375\n",
      "Epoch 323/1000\n",
      "72/72 [==============================] - 2s 23ms/step - loss: 0.6374 - val_loss: 0.6373\n",
      "Epoch 324/1000\n",
      "72/72 [==============================] - 2s 24ms/step - loss: 0.6375 - val_loss: 0.6374\n",
      "Epoch 325/1000\n",
      "72/72 [==============================] - 2s 23ms/step - loss: 0.6375 - val_loss: 0.6375\n",
      "Epoch 326/1000\n",
      "72/72 [==============================] - 2s 24ms/step - loss: 0.6375 - val_loss: 0.6379\n",
      "Epoch 327/1000\n",
      "72/72 [==============================] - 2s 24ms/step - loss: 0.6376 - val_loss: 0.6372\n",
      "Epoch 328/1000\n",
      "72/72 [==============================] - 2s 24ms/step - loss: 0.6374 - val_loss: 0.6373\n",
      "Epoch 329/1000\n",
      "72/72 [==============================] - 2s 23ms/step - loss: 0.6375 - val_loss: 0.6374\n",
      "Epoch 330/1000\n",
      "72/72 [==============================] - 2s 23ms/step - loss: 0.6376 - val_loss: 0.6377\n",
      "Epoch 331/1000\n",
      "72/72 [==============================] - 2s 23ms/step - loss: 0.6376 - val_loss: 0.6374\n",
      "Epoch 332/1000\n",
      "72/72 [==============================] - 2s 24ms/step - loss: 0.6374 - val_loss: 0.6372\n",
      "Epoch 333/1000\n",
      "72/72 [==============================] - 2s 23ms/step - loss: 0.6376 - val_loss: 0.6375\n",
      "Epoch 334/1000\n",
      "72/72 [==============================] - 2s 23ms/step - loss: 0.6375 - val_loss: 0.6385\n",
      "Epoch 335/1000\n",
      "72/72 [==============================] - 2s 23ms/step - loss: 0.6376 - val_loss: 0.6370\n",
      "Epoch 336/1000\n",
      "72/72 [==============================] - 2s 24ms/step - loss: 0.6370 - val_loss: 0.6372\n",
      "Epoch 337/1000\n",
      "72/72 [==============================] - 2s 24ms/step - loss: 0.6367 - val_loss: 0.6365\n",
      "Epoch 338/1000\n",
      "72/72 [==============================] - 2s 24ms/step - loss: 0.6369 - val_loss: 0.6373\n",
      "Epoch 339/1000\n",
      "72/72 [==============================] - 2s 23ms/step - loss: 0.6371 - val_loss: 0.6376\n",
      "Epoch 340/1000\n",
      "72/72 [==============================] - 2s 24ms/step - loss: 0.6369 - val_loss: 0.6370\n",
      "Epoch 341/1000\n",
      "72/72 [==============================] - 2s 25ms/step - loss: 0.6369 - val_loss: 0.6366\n",
      "Epoch 342/1000\n",
      "72/72 [==============================] - 2s 27ms/step - loss: 0.6366 - val_loss: 0.6369\n",
      "Epoch 343/1000\n",
      "72/72 [==============================] - 2s 26ms/step - loss: 0.6368 - val_loss: 0.6375\n",
      "Epoch 344/1000\n",
      "72/72 [==============================] - 2s 26ms/step - loss: 0.6375 - val_loss: 0.6375\n",
      "Epoch 345/1000\n",
      "72/72 [==============================] - 2s 25ms/step - loss: 0.6375 - val_loss: 0.6374\n",
      "Epoch 346/1000\n",
      "72/72 [==============================] - 2s 25ms/step - loss: 0.6375 - val_loss: 0.6375\n",
      "Epoch 347/1000\n",
      "72/72 [==============================] - 2s 24ms/step - loss: 0.6374 - val_loss: 0.6374\n",
      "Epoch 348/1000\n",
      "72/72 [==============================] - 2s 24ms/step - loss: 0.6375 - val_loss: 0.6369\n",
      "Epoch 349/1000\n",
      "72/72 [==============================] - 2s 23ms/step - loss: 0.6371 - val_loss: 0.6371\n",
      "Epoch 350/1000\n",
      "72/72 [==============================] - 2s 26ms/step - loss: 0.6373 - val_loss: 0.6374\n",
      "Epoch 351/1000\n",
      "72/72 [==============================] - 2s 27ms/step - loss: 0.6373 - val_loss: 0.6375\n",
      "Epoch 352/1000\n",
      "72/72 [==============================] - 2s 24ms/step - loss: 0.6375 - val_loss: 0.6373\n",
      "Epoch 353/1000\n",
      "72/72 [==============================] - 2s 25ms/step - loss: 0.6374 - val_loss: 0.6373\n",
      "Epoch 354/1000\n",
      "72/72 [==============================] - 2s 23ms/step - loss: 0.6374 - val_loss: 0.6374\n",
      "Epoch 355/1000\n",
      "72/72 [==============================] - 2s 24ms/step - loss: 0.6374 - val_loss: 0.6373\n",
      "Epoch 356/1000\n",
      "72/72 [==============================] - 2s 22ms/step - loss: 0.6375 - val_loss: 0.6392\n",
      "Epoch 357/1000\n",
      "72/72 [==============================] - 2s 23ms/step - loss: 0.6376 - val_loss: 0.6370\n",
      "Epoch 358/1000\n",
      "72/72 [==============================] - 2s 24ms/step - loss: 0.6359 - val_loss: 0.6360\n",
      "Epoch 359/1000\n",
      "72/72 [==============================] - 2s 23ms/step - loss: 0.6363 - val_loss: 0.6356\n",
      "Epoch 360/1000\n",
      "72/72 [==============================] - 2s 25ms/step - loss: 0.6372 - val_loss: 0.6366\n",
      "Epoch 361/1000\n",
      "72/72 [==============================] - 2s 24ms/step - loss: 0.6372 - val_loss: 0.6376\n",
      "Epoch 362/1000\n",
      "72/72 [==============================] - 2s 24ms/step - loss: 0.6380 - val_loss: 0.6376\n",
      "Epoch 363/1000\n",
      "72/72 [==============================] - 2s 24ms/step - loss: 0.6366 - val_loss: 0.6367\n",
      "Epoch 364/1000\n",
      "72/72 [==============================] - 2s 24ms/step - loss: 0.6363 - val_loss: 0.6358\n",
      "Epoch 365/1000\n",
      "72/72 [==============================] - 2s 29ms/step - loss: 0.6349 - val_loss: 0.6346\n",
      "Epoch 366/1000\n",
      "72/72 [==============================] - 2s 26ms/step - loss: 0.6364 - val_loss: 0.6359\n",
      "Epoch 367/1000\n",
      "72/72 [==============================] - 2s 24ms/step - loss: 0.6354 - val_loss: 0.6369\n",
      "Epoch 368/1000\n",
      "72/72 [==============================] - 2s 24ms/step - loss: 0.6360 - val_loss: 0.6357\n",
      "Epoch 369/1000\n",
      "72/72 [==============================] - 2s 24ms/step - loss: 0.6355 - val_loss: 0.6353\n",
      "Epoch 370/1000\n",
      "72/72 [==============================] - 2s 24ms/step - loss: 0.6352 - val_loss: 0.6350\n",
      "Epoch 371/1000\n",
      "72/72 [==============================] - 2s 24ms/step - loss: 0.6349 - val_loss: 0.6348\n",
      "Epoch 372/1000\n",
      "72/72 [==============================] - 2s 25ms/step - loss: 0.6345 - val_loss: 0.6340\n",
      "Epoch 373/1000\n",
      "72/72 [==============================] - 2s 25ms/step - loss: 0.6345 - val_loss: 0.6352\n",
      "Epoch 374/1000\n",
      "72/72 [==============================] - 2s 24ms/step - loss: 0.6342 - val_loss: 0.6340\n",
      "Epoch 375/1000\n",
      "72/72 [==============================] - 2s 24ms/step - loss: 0.6348 - val_loss: 0.6358\n",
      "Epoch 376/1000\n",
      "72/72 [==============================] - 2s 23ms/step - loss: 0.6342 - val_loss: 0.6345\n",
      "Epoch 377/1000\n",
      "72/72 [==============================] - 2s 22ms/step - loss: 0.6344 - val_loss: 0.6339\n",
      "Epoch 378/1000\n",
      "72/72 [==============================] - 2s 23ms/step - loss: 0.6343 - val_loss: 0.6370\n",
      "Epoch 379/1000\n",
      "72/72 [==============================] - 2s 23ms/step - loss: 0.6346 - val_loss: 0.6345\n",
      "Epoch 380/1000\n",
      "72/72 [==============================] - 2s 23ms/step - loss: 0.6347 - val_loss: 0.6337\n",
      "Epoch 381/1000\n",
      "72/72 [==============================] - 2s 23ms/step - loss: 0.6340 - val_loss: 0.6350\n",
      "Epoch 382/1000\n",
      "72/72 [==============================] - 2s 22ms/step - loss: 0.6352 - val_loss: 0.6341\n",
      "Epoch 383/1000\n",
      "72/72 [==============================] - 2s 23ms/step - loss: 0.6339 - val_loss: 0.6335\n",
      "Epoch 384/1000\n",
      "72/72 [==============================] - 2s 23ms/step - loss: 0.6340 - val_loss: 0.6334\n",
      "Epoch 385/1000\n",
      "72/72 [==============================] - 2s 24ms/step - loss: 0.6343 - val_loss: 0.6353\n",
      "Epoch 386/1000\n",
      "72/72 [==============================] - 2s 25ms/step - loss: 0.6353 - val_loss: 0.6361\n",
      "Epoch 387/1000\n",
      "72/72 [==============================] - 2s 25ms/step - loss: 0.6363 - val_loss: 0.6376\n",
      "Epoch 388/1000\n",
      "72/72 [==============================] - 2s 28ms/step - loss: 0.6368 - val_loss: 0.6355\n",
      "Epoch 389/1000\n",
      "72/72 [==============================] - 2s 27ms/step - loss: 0.6367 - val_loss: 0.6372\n",
      "Epoch 390/1000\n",
      "72/72 [==============================] - 2s 25ms/step - loss: 0.6364 - val_loss: 0.6354\n",
      "Epoch 391/1000\n",
      "72/72 [==============================] - 2s 25ms/step - loss: 0.6348 - val_loss: 0.6356\n",
      "Epoch 392/1000\n",
      "72/72 [==============================] - 2s 25ms/step - loss: 0.6341 - val_loss: 0.6339\n",
      "Epoch 393/1000\n",
      "72/72 [==============================] - 2s 24ms/step - loss: 0.6358 - val_loss: 0.6479\n",
      "Epoch 394/1000\n",
      "72/72 [==============================] - 2s 24ms/step - loss: 0.6372 - val_loss: 0.6358\n",
      "Epoch 395/1000\n",
      "72/72 [==============================] - 2s 24ms/step - loss: 0.6344 - val_loss: 0.6345\n",
      "Epoch 396/1000\n",
      "72/72 [==============================] - 2s 24ms/step - loss: 0.6341 - val_loss: 0.6337\n",
      "Epoch 397/1000\n",
      "72/72 [==============================] - 2s 24ms/step - loss: 0.6339 - val_loss: 0.6339\n",
      "Epoch 398/1000\n",
      "72/72 [==============================] - 2s 23ms/step - loss: 0.6339 - val_loss: 0.6340\n",
      "Epoch 399/1000\n",
      "72/72 [==============================] - 2s 24ms/step - loss: 0.6341 - val_loss: 0.6339\n",
      "Epoch 400/1000\n",
      "72/72 [==============================] - 2s 25ms/step - loss: 0.6339 - val_loss: 0.6338\n",
      "Epoch 401/1000\n",
      "72/72 [==============================] - 2s 25ms/step - loss: 0.6338 - val_loss: 0.6337\n",
      "Epoch 402/1000\n",
      "72/72 [==============================] - 2s 23ms/step - loss: 0.6346 - val_loss: 0.6349\n",
      "Epoch 403/1000\n",
      "72/72 [==============================] - 2s 25ms/step - loss: 0.6354 - val_loss: 0.6359\n",
      "Epoch 404/1000\n",
      "72/72 [==============================] - 2s 24ms/step - loss: 0.6350 - val_loss: 0.6345\n",
      "Epoch 405/1000\n",
      "72/72 [==============================] - 2s 23ms/step - loss: 0.6347 - val_loss: 0.6345\n",
      "Epoch 406/1000\n",
      "72/72 [==============================] - 2s 24ms/step - loss: 0.6344 - val_loss: 0.6340\n",
      "Epoch 407/1000\n",
      "72/72 [==============================] - 2s 24ms/step - loss: 0.6338 - val_loss: 0.6341\n",
      "Epoch 408/1000\n",
      "72/72 [==============================] - 2s 23ms/step - loss: 0.6342 - val_loss: 0.6339\n",
      "Epoch 409/1000\n",
      "72/72 [==============================] - 2s 23ms/step - loss: 0.6371 - val_loss: 0.6388\n",
      "Epoch 410/1000\n",
      "72/72 [==============================] - 2s 25ms/step - loss: 0.6369 - val_loss: 0.6358\n",
      "Epoch 411/1000\n",
      "72/72 [==============================] - 2s 23ms/step - loss: 0.6350 - val_loss: 0.6344\n",
      "Epoch 412/1000\n",
      "72/72 [==============================] - 2s 25ms/step - loss: 0.6345 - val_loss: 0.6338\n",
      "Epoch 413/1000\n",
      "72/72 [==============================] - 2s 23ms/step - loss: 0.6338 - val_loss: 0.6339\n",
      "Epoch 414/1000\n",
      "72/72 [==============================] - 2s 25ms/step - loss: 0.6339 - val_loss: 0.6336\n",
      "Epoch 415/1000\n",
      "72/72 [==============================] - 2s 24ms/step - loss: 0.6341 - val_loss: 0.6345\n",
      "Epoch 416/1000\n",
      "72/72 [==============================] - 2s 23ms/step - loss: 0.6347 - val_loss: 0.6347\n",
      "Epoch 417/1000\n",
      "72/72 [==============================] - 2s 24ms/step - loss: 0.6348 - val_loss: 0.6350\n",
      "Epoch 418/1000\n",
      "72/72 [==============================] - 2s 23ms/step - loss: 0.6351 - val_loss: 0.6345\n",
      "Epoch 419/1000\n",
      "72/72 [==============================] - 2s 23ms/step - loss: 0.6345 - val_loss: 0.6340\n",
      "Epoch 420/1000\n",
      "72/72 [==============================] - 2s 24ms/step - loss: 0.6337 - val_loss: 0.6332\n",
      "Epoch 421/1000\n",
      "72/72 [==============================] - 2s 24ms/step - loss: 0.6335 - val_loss: 0.6332\n",
      "Epoch 422/1000\n",
      "72/72 [==============================] - 2s 24ms/step - loss: 0.6330 - val_loss: 0.6334\n",
      "Epoch 423/1000\n",
      "72/72 [==============================] - 2s 25ms/step - loss: 0.6347 - val_loss: 0.6344\n",
      "Epoch 424/1000\n",
      "72/72 [==============================] - 2s 23ms/step - loss: 0.6341 - val_loss: 0.6338\n",
      "Epoch 425/1000\n",
      "72/72 [==============================] - 2s 24ms/step - loss: 0.6336 - val_loss: 0.6341\n",
      "Epoch 426/1000\n",
      "72/72 [==============================] - 2s 28ms/step - loss: 0.6341 - val_loss: 0.6336\n",
      "Epoch 427/1000\n",
      "72/72 [==============================] - 2s 27ms/step - loss: 0.6337 - val_loss: 0.6337\n",
      "Epoch 428/1000\n",
      "72/72 [==============================] - 2s 28ms/step - loss: 0.6338 - val_loss: 0.6335\n",
      "Epoch 429/1000\n",
      "72/72 [==============================] - 2s 26ms/step - loss: 0.6336 - val_loss: 0.6334\n",
      "Epoch 430/1000\n",
      "72/72 [==============================] - 2s 25ms/step - loss: 0.6335 - val_loss: 0.6331\n",
      "Epoch 431/1000\n",
      "72/72 [==============================] - 2s 28ms/step - loss: 0.6334 - val_loss: 0.6331\n",
      "Epoch 432/1000\n",
      "72/72 [==============================] - 2s 26ms/step - loss: 0.6338 - val_loss: 0.6334\n",
      "Epoch 433/1000\n",
      "72/72 [==============================] - 2s 29ms/step - loss: 0.6333 - val_loss: 0.6330\n",
      "Epoch 434/1000\n",
      "72/72 [==============================] - 2s 24ms/step - loss: 0.6336 - val_loss: 0.6338\n",
      "Epoch 435/1000\n",
      "72/72 [==============================] - 2s 24ms/step - loss: 0.6352 - val_loss: 0.6346\n",
      "Epoch 436/1000\n",
      "72/72 [==============================] - 2s 27ms/step - loss: 0.6343 - val_loss: 0.6346\n",
      "Epoch 437/1000\n",
      "72/72 [==============================] - 2s 26ms/step - loss: 0.6342 - val_loss: 0.6340\n",
      "Epoch 438/1000\n",
      "72/72 [==============================] - 2s 24ms/step - loss: 0.6345 - val_loss: 0.6342\n",
      "Epoch 439/1000\n",
      "72/72 [==============================] - 2s 26ms/step - loss: 0.6342 - val_loss: 0.6340\n",
      "Epoch 440/1000\n",
      "72/72 [==============================] - 2s 23ms/step - loss: 0.6336 - val_loss: 0.6334\n",
      "Epoch 441/1000\n",
      "72/72 [==============================] - 2s 24ms/step - loss: 0.6332 - val_loss: 0.6332\n",
      "Epoch 442/1000\n",
      "72/72 [==============================] - 2s 24ms/step - loss: 0.6338 - val_loss: 0.6342\n",
      "Epoch 443/1000\n",
      "72/72 [==============================] - 2s 25ms/step - loss: 0.6337 - val_loss: 0.6335\n",
      "Epoch 444/1000\n",
      "72/72 [==============================] - 2s 23ms/step - loss: 0.6334 - val_loss: 0.6332\n",
      "Epoch 445/1000\n",
      "72/72 [==============================] - 2s 22ms/step - loss: 0.6328 - val_loss: 0.6324\n",
      "Epoch 446/1000\n",
      "72/72 [==============================] - 2s 23ms/step - loss: 0.6327 - val_loss: 0.6334\n",
      "Epoch 447/1000\n",
      "72/72 [==============================] - 2s 24ms/step - loss: 0.6327 - val_loss: 0.6335\n",
      "Epoch 448/1000\n",
      "72/72 [==============================] - 2s 23ms/step - loss: 0.6343 - val_loss: 0.6337\n",
      "Epoch 449/1000\n",
      "72/72 [==============================] - 2s 24ms/step - loss: 0.6335 - val_loss: 0.6334\n",
      "Epoch 450/1000\n",
      "72/72 [==============================] - 2s 23ms/step - loss: 0.6348 - val_loss: 0.6346\n",
      "Epoch 451/1000\n",
      "72/72 [==============================] - 2s 24ms/step - loss: 0.6341 - val_loss: 0.6344\n",
      "Epoch 452/1000\n",
      "72/72 [==============================] - 2s 24ms/step - loss: 0.6351 - val_loss: 0.6349\n",
      "Epoch 453/1000\n",
      "72/72 [==============================] - 2s 23ms/step - loss: 0.6353 - val_loss: 0.6349\n",
      "Epoch 454/1000\n",
      "72/72 [==============================] - 2s 25ms/step - loss: 0.6347 - val_loss: 0.6343\n",
      "Epoch 455/1000\n",
      "72/72 [==============================] - 2s 23ms/step - loss: 0.6342 - val_loss: 0.6335\n",
      "Epoch 456/1000\n",
      "72/72 [==============================] - 2s 24ms/step - loss: 0.6335 - val_loss: 0.6333\n",
      "Epoch 457/1000\n",
      "72/72 [==============================] - 2s 23ms/step - loss: 0.6335 - val_loss: 0.6332\n",
      "Epoch 458/1000\n",
      "72/72 [==============================] - 2s 23ms/step - loss: 0.6334 - val_loss: 0.6334\n",
      "Epoch 459/1000\n",
      "72/72 [==============================] - 2s 23ms/step - loss: 0.6331 - val_loss: 0.6330\n",
      "Epoch 460/1000\n",
      "72/72 [==============================] - 2s 24ms/step - loss: 0.6332 - val_loss: 0.6329\n",
      "Epoch 461/1000\n",
      "72/72 [==============================] - 2s 23ms/step - loss: 0.6332 - val_loss: 0.6332\n",
      "Epoch 462/1000\n",
      "72/72 [==============================] - 2s 23ms/step - loss: 0.6331 - val_loss: 0.6331\n",
      "Epoch 463/1000\n",
      "72/72 [==============================] - 2s 23ms/step - loss: 0.6328 - val_loss: 0.6328\n",
      "Epoch 464/1000\n",
      "72/72 [==============================] - 2s 24ms/step - loss: 0.6336 - val_loss: 0.6333\n",
      "Epoch 465/1000\n",
      "72/72 [==============================] - 2s 23ms/step - loss: 0.6339 - val_loss: 0.6334\n",
      "Epoch 466/1000\n",
      "72/72 [==============================] - 2s 25ms/step - loss: 0.6335 - val_loss: 0.6328\n",
      "Epoch 467/1000\n",
      "72/72 [==============================] - 2s 23ms/step - loss: 0.6332 - val_loss: 0.6336\n",
      "Epoch 468/1000\n",
      "72/72 [==============================] - 2s 23ms/step - loss: 0.6336 - val_loss: 0.6333\n",
      "Epoch 469/1000\n",
      "72/72 [==============================] - 2s 23ms/step - loss: 0.6334 - val_loss: 0.6336\n",
      "Epoch 470/1000\n",
      "72/72 [==============================] - 2s 23ms/step - loss: 0.6332 - val_loss: 0.6329\n",
      "Epoch 471/1000\n",
      "72/72 [==============================] - 2s 23ms/step - loss: 0.6329 - val_loss: 0.6334\n",
      "Epoch 472/1000\n",
      "72/72 [==============================] - 2s 24ms/step - loss: 0.6330 - val_loss: 0.6332\n",
      "Epoch 473/1000\n",
      "72/72 [==============================] - 2s 23ms/step - loss: 0.6332 - val_loss: 0.6331\n",
      "Epoch 474/1000\n",
      "72/72 [==============================] - 2s 24ms/step - loss: 0.6330 - val_loss: 0.6333\n",
      "Epoch 475/1000\n",
      "72/72 [==============================] - 2s 24ms/step - loss: 0.6359 - val_loss: 0.6337\n",
      "Epoch 476/1000\n",
      "72/72 [==============================] - 2s 24ms/step - loss: 0.6333 - val_loss: 0.6338\n",
      "Epoch 477/1000\n",
      "72/72 [==============================] - 2s 24ms/step - loss: 0.6337 - val_loss: 0.6331\n",
      "Epoch 478/1000\n",
      "72/72 [==============================] - 2s 24ms/step - loss: 0.6329 - val_loss: 0.6328\n",
      "Epoch 479/1000\n",
      "72/72 [==============================] - 2s 23ms/step - loss: 0.6329 - val_loss: 0.6326\n",
      "Epoch 480/1000\n",
      "72/72 [==============================] - 2s 24ms/step - loss: 0.6328 - val_loss: 0.6328\n",
      "Epoch 481/1000\n",
      "72/72 [==============================] - 2s 24ms/step - loss: 0.6331 - val_loss: 0.6330\n",
      "Epoch 482/1000\n",
      "72/72 [==============================] - 2s 23ms/step - loss: 0.6333 - val_loss: 0.6340\n",
      "Epoch 483/1000\n",
      "72/72 [==============================] - 2s 24ms/step - loss: 0.6364 - val_loss: 0.6378\n",
      "Epoch 484/1000\n",
      "72/72 [==============================] - 2s 23ms/step - loss: 0.6394 - val_loss: 0.6395\n",
      "Epoch 485/1000\n",
      "72/72 [==============================] - 2s 24ms/step - loss: 0.6396 - val_loss: 0.6390\n",
      "Epoch 486/1000\n",
      "72/72 [==============================] - 2s 25ms/step - loss: 0.6390 - val_loss: 0.6386\n",
      "Epoch 487/1000\n",
      "72/72 [==============================] - 2s 23ms/step - loss: 0.6390 - val_loss: 0.6387\n",
      "Epoch 488/1000\n",
      "72/72 [==============================] - 2s 24ms/step - loss: 0.6385 - val_loss: 0.6387\n",
      "Epoch 489/1000\n",
      "72/72 [==============================] - 2s 23ms/step - loss: 0.6385 - val_loss: 0.6383\n",
      "Epoch 490/1000\n",
      "72/72 [==============================] - 2s 24ms/step - loss: 0.6384 - val_loss: 0.6383\n",
      "Epoch 491/1000\n",
      "72/72 [==============================] - 2s 23ms/step - loss: 0.6383 - val_loss: 0.6380\n",
      "Epoch 492/1000\n",
      "72/72 [==============================] - 2s 24ms/step - loss: 0.6382 - val_loss: 0.6383\n",
      "Epoch 493/1000\n",
      "72/72 [==============================] - 2s 25ms/step - loss: 0.6381 - val_loss: 0.6379\n",
      "Epoch 494/1000\n",
      "72/72 [==============================] - 2s 23ms/step - loss: 0.6379 - val_loss: 0.6377\n",
      "Epoch 495/1000\n",
      "72/72 [==============================] - 2s 25ms/step - loss: 0.6362 - val_loss: 0.6349\n",
      "Epoch 496/1000\n",
      "72/72 [==============================] - 2s 24ms/step - loss: 0.6350 - val_loss: 0.6343\n",
      "Epoch 497/1000\n",
      "72/72 [==============================] - 2s 25ms/step - loss: 0.6345 - val_loss: 0.6337\n",
      "Epoch 498/1000\n",
      "72/72 [==============================] - 2s 28ms/step - loss: 0.6347 - val_loss: 0.6362\n",
      "Epoch 499/1000\n",
      "72/72 [==============================] - 2s 24ms/step - loss: 0.6362 - val_loss: 0.6367\n",
      "Epoch 500/1000\n",
      "72/72 [==============================] - 2s 25ms/step - loss: 0.6358 - val_loss: 0.6343\n",
      "Epoch 501/1000\n",
      "72/72 [==============================] - 2s 24ms/step - loss: 0.6348 - val_loss: 0.6350\n",
      "Epoch 502/1000\n",
      "72/72 [==============================] - 2s 24ms/step - loss: 0.6343 - val_loss: 0.6343\n",
      "Epoch 503/1000\n",
      "72/72 [==============================] - 2s 24ms/step - loss: 0.6399 - val_loss: 0.6408\n",
      "Epoch 504/1000\n",
      "72/72 [==============================] - 2s 24ms/step - loss: 0.6393 - val_loss: 0.6392\n",
      "Epoch 505/1000\n",
      "72/72 [==============================] - 2s 23ms/step - loss: 0.6385 - val_loss: 0.6381\n",
      "Epoch 506/1000\n",
      "72/72 [==============================] - 2s 23ms/step - loss: 0.6381 - val_loss: 0.6380\n",
      "Epoch 507/1000\n",
      "72/72 [==============================] - 2s 24ms/step - loss: 0.6381 - val_loss: 0.6379\n",
      "Epoch 508/1000\n",
      "72/72 [==============================] - 2s 24ms/step - loss: 0.6378 - val_loss: 0.6379\n",
      "Epoch 509/1000\n",
      "72/72 [==============================] - 2s 24ms/step - loss: 0.6378 - val_loss: 0.6378\n",
      "Epoch 510/1000\n",
      "72/72 [==============================] - 2s 25ms/step - loss: 0.6378 - val_loss: 0.6377\n",
      "Epoch 511/1000\n",
      "72/72 [==============================] - 2s 24ms/step - loss: 0.6377 - val_loss: 0.6376\n",
      "Epoch 512/1000\n",
      "72/72 [==============================] - 2s 24ms/step - loss: 0.6377 - val_loss: 0.6378\n",
      "Epoch 513/1000\n",
      "72/72 [==============================] - 2s 23ms/step - loss: 0.6377 - val_loss: 0.6376\n",
      "Epoch 514/1000\n",
      "72/72 [==============================] - 2s 23ms/step - loss: 0.6378 - val_loss: 0.6379\n",
      "Epoch 515/1000\n",
      "72/72 [==============================] - 2s 22ms/step - loss: 0.6376 - val_loss: 0.6376\n",
      "Epoch 516/1000\n",
      "72/72 [==============================] - 2s 23ms/step - loss: 0.6388 - val_loss: 0.6385\n",
      "Epoch 517/1000\n",
      "72/72 [==============================] - 2s 23ms/step - loss: 0.6352 - val_loss: 0.6342\n",
      "Epoch 518/1000\n",
      "72/72 [==============================] - 2s 22ms/step - loss: 0.6351 - val_loss: 0.6343\n",
      "Epoch 519/1000\n",
      "72/72 [==============================] - 2s 24ms/step - loss: 0.6344 - val_loss: 0.6348\n",
      "Epoch 520/1000\n",
      "72/72 [==============================] - 2s 23ms/step - loss: 0.6341 - val_loss: 0.6337\n",
      "Epoch 521/1000\n",
      "72/72 [==============================] - 2s 24ms/step - loss: 0.6349 - val_loss: 0.6341\n",
      "Epoch 522/1000\n",
      "72/72 [==============================] - 2s 24ms/step - loss: 0.6363 - val_loss: 0.6369\n",
      "Epoch 523/1000\n",
      "72/72 [==============================] - 2s 24ms/step - loss: 0.6353 - val_loss: 0.6343\n",
      "Epoch 524/1000\n",
      "72/72 [==============================] - 2s 23ms/step - loss: 0.6342 - val_loss: 0.6344\n",
      "Epoch 525/1000\n",
      "72/72 [==============================] - 2s 23ms/step - loss: 0.6338 - val_loss: 0.6335\n",
      "Epoch 526/1000\n",
      "72/72 [==============================] - 2s 23ms/step - loss: 0.6334 - val_loss: 0.6337\n",
      "Epoch 527/1000\n",
      "72/72 [==============================] - 2s 24ms/step - loss: 0.6335 - val_loss: 0.6332\n",
      "Epoch 528/1000\n",
      "72/72 [==============================] - 2s 23ms/step - loss: 0.6337 - val_loss: 0.6341\n",
      "Epoch 529/1000\n",
      "72/72 [==============================] - 2s 24ms/step - loss: 0.6337 - val_loss: 0.6334\n",
      "Epoch 530/1000\n",
      "72/72 [==============================] - 2s 24ms/step - loss: 0.6332 - val_loss: 0.6328\n",
      "Epoch 531/1000\n",
      "72/72 [==============================] - 2s 23ms/step - loss: 0.6333 - val_loss: 0.6341\n",
      "Epoch 532/1000\n",
      "72/72 [==============================] - 2s 23ms/step - loss: 0.6338 - val_loss: 0.6330\n",
      "Epoch 533/1000\n",
      "72/72 [==============================] - 2s 23ms/step - loss: 0.6341 - val_loss: 0.6339\n",
      "Epoch 534/1000\n",
      "72/72 [==============================] - 2s 24ms/step - loss: 0.6332 - val_loss: 0.6339\n",
      "Epoch 535/1000\n",
      "72/72 [==============================] - 2s 24ms/step - loss: 0.6335 - val_loss: 0.6330\n",
      "Epoch 536/1000\n",
      "72/72 [==============================] - 2s 25ms/step - loss: 0.6332 - val_loss: 0.6329\n",
      "Epoch 537/1000\n",
      "72/72 [==============================] - 2s 26ms/step - loss: 0.6330 - val_loss: 0.6332\n",
      "Epoch 538/1000\n",
      "72/72 [==============================] - 2s 27ms/step - loss: 0.6330 - val_loss: 0.6331\n",
      "Epoch 539/1000\n",
      "72/72 [==============================] - 2s 24ms/step - loss: 0.6330 - val_loss: 0.6327\n",
      "Epoch 540/1000\n",
      "72/72 [==============================] - 2s 26ms/step - loss: 0.6330 - val_loss: 0.6333\n",
      "Epoch 541/1000\n",
      "72/72 [==============================] - 2s 24ms/step - loss: 0.6343 - val_loss: 0.6335\n",
      "Epoch 542/1000\n",
      "72/72 [==============================] - 2s 24ms/step - loss: 0.6334 - val_loss: 0.6331\n",
      "Epoch 543/1000\n",
      "72/72 [==============================] - 2s 24ms/step - loss: 0.6335 - val_loss: 0.6328\n",
      "Epoch 544/1000\n",
      "72/72 [==============================] - 2s 25ms/step - loss: 0.6328 - val_loss: 0.6332\n",
      "Epoch 545/1000\n",
      "72/72 [==============================] - 2s 24ms/step - loss: 0.6334 - val_loss: 0.6331\n",
      "Epoch 546/1000\n",
      "72/72 [==============================] - 2s 23ms/step - loss: 0.6332 - val_loss: 0.6334\n",
      "Epoch 547/1000\n",
      "72/72 [==============================] - 2s 24ms/step - loss: 0.6335 - val_loss: 0.6337\n",
      "Epoch 548/1000\n",
      "72/72 [==============================] - 2s 24ms/step - loss: 0.6335 - val_loss: 0.6334\n",
      "Epoch 549/1000\n",
      "72/72 [==============================] - 2s 24ms/step - loss: 0.6335 - val_loss: 0.6335\n",
      "Epoch 550/1000\n",
      "72/72 [==============================] - 2s 24ms/step - loss: 0.6335 - val_loss: 0.6332\n",
      "Epoch 551/1000\n",
      "72/72 [==============================] - 2s 24ms/step - loss: 0.6336 - val_loss: 0.6341\n",
      "Epoch 552/1000\n",
      "72/72 [==============================] - 2s 24ms/step - loss: 0.6344 - val_loss: 0.6340\n",
      "Epoch 553/1000\n",
      "72/72 [==============================] - 2s 25ms/step - loss: 0.6344 - val_loss: 0.6344\n",
      "Epoch 554/1000\n",
      "72/72 [==============================] - 2s 23ms/step - loss: 0.6343 - val_loss: 0.6338\n",
      "Epoch 555/1000\n",
      "72/72 [==============================] - 2s 24ms/step - loss: 0.6340 - val_loss: 0.6345\n",
      "Epoch 556/1000\n",
      "72/72 [==============================] - 2s 24ms/step - loss: 0.6349 - val_loss: 0.6353\n",
      "Epoch 557/1000\n",
      "72/72 [==============================] - 2s 24ms/step - loss: 0.6351 - val_loss: 0.6360\n",
      "Epoch 558/1000\n",
      "72/72 [==============================] - 2s 25ms/step - loss: 0.6346 - val_loss: 0.6343\n",
      "Epoch 559/1000\n",
      "72/72 [==============================] - 2s 23ms/step - loss: 0.6347 - val_loss: 0.6351\n",
      "Epoch 560/1000\n",
      "72/72 [==============================] - 2s 24ms/step - loss: 0.6349 - val_loss: 0.6344\n",
      "Epoch 561/1000\n",
      "72/72 [==============================] - 2s 25ms/step - loss: 0.6346 - val_loss: 0.6363\n",
      "Epoch 562/1000\n",
      "72/72 [==============================] - 2s 25ms/step - loss: 0.6369 - val_loss: 0.6373\n",
      "Epoch 563/1000\n",
      "72/72 [==============================] - 2s 27ms/step - loss: 0.6363 - val_loss: 0.6363\n",
      "Epoch 564/1000\n",
      "72/72 [==============================] - 2s 25ms/step - loss: 0.6363 - val_loss: 0.6374\n",
      "Epoch 565/1000\n",
      "72/72 [==============================] - 2s 25ms/step - loss: 0.6374 - val_loss: 0.6370\n",
      "Epoch 566/1000\n",
      "72/72 [==============================] - 2s 25ms/step - loss: 0.6369 - val_loss: 0.6366\n",
      "Epoch 567/1000\n",
      "72/72 [==============================] - 2s 23ms/step - loss: 0.6357 - val_loss: 0.6351\n",
      "Epoch 568/1000\n",
      "72/72 [==============================] - 2s 25ms/step - loss: 0.6350 - val_loss: 0.6348\n",
      "Epoch 569/1000\n",
      "72/72 [==============================] - 2s 25ms/step - loss: 0.6355 - val_loss: 0.6355\n",
      "Epoch 570/1000\n",
      "72/72 [==============================] - 2s 24ms/step - loss: 0.6353 - val_loss: 0.6349\n",
      "Epoch 571/1000\n",
      "72/72 [==============================] - 2s 24ms/step - loss: 0.6358 - val_loss: 0.6363\n",
      "Epoch 572/1000\n",
      "72/72 [==============================] - 2s 24ms/step - loss: 0.6356 - val_loss: 0.6351\n",
      "Epoch 573/1000\n",
      "72/72 [==============================] - 2s 24ms/step - loss: 0.6349 - val_loss: 0.6349\n",
      "Epoch 574/1000\n",
      "72/72 [==============================] - 2s 25ms/step - loss: 0.6346 - val_loss: 0.6356\n",
      "Epoch 575/1000\n",
      "72/72 [==============================] - 2s 23ms/step - loss: 0.6356 - val_loss: 0.6365\n",
      "Epoch 576/1000\n",
      "72/72 [==============================] - 2s 24ms/step - loss: 0.6372 - val_loss: 0.6376\n",
      "Epoch 577/1000\n",
      "72/72 [==============================] - 2s 24ms/step - loss: 0.6378 - val_loss: 0.6380\n",
      "Epoch 578/1000\n",
      "72/72 [==============================] - 2s 23ms/step - loss: 0.6382 - val_loss: 0.6383\n",
      "Epoch 579/1000\n",
      "72/72 [==============================] - 2s 23ms/step - loss: 0.6381 - val_loss: 0.6359\n",
      "Epoch 580/1000\n",
      "72/72 [==============================] - 2s 23ms/step - loss: 0.6364 - val_loss: 0.6361\n",
      "Epoch 581/1000\n",
      "72/72 [==============================] - 2s 24ms/step - loss: 0.6359 - val_loss: 0.6356\n",
      "Epoch 582/1000\n",
      "72/72 [==============================] - 2s 24ms/step - loss: 0.6367 - val_loss: 0.6357\n",
      "Epoch 583/1000\n",
      "72/72 [==============================] - 2s 23ms/step - loss: 0.6357 - val_loss: 0.6363\n",
      "Epoch 584/1000\n",
      "72/72 [==============================] - 2s 23ms/step - loss: 0.6369 - val_loss: 0.6381\n",
      "Epoch 585/1000\n",
      "72/72 [==============================] - 2s 23ms/step - loss: 0.6381 - val_loss: 0.6381\n",
      "Epoch 586/1000\n",
      "72/72 [==============================] - 2s 24ms/step - loss: 0.6382 - val_loss: 0.6381\n",
      "Epoch 587/1000\n",
      "72/72 [==============================] - 2s 23ms/step - loss: 0.6367 - val_loss: 0.6366\n",
      "Epoch 588/1000\n",
      "72/72 [==============================] - 2s 24ms/step - loss: 0.6368 - val_loss: 0.6363\n",
      "Epoch 589/1000\n",
      "72/72 [==============================] - 2s 23ms/step - loss: 0.6358 - val_loss: 0.6355\n",
      "Epoch 590/1000\n",
      "72/72 [==============================] - 2s 23ms/step - loss: 0.6359 - val_loss: 0.6361\n",
      "Epoch 591/1000\n",
      "72/72 [==============================] - 2s 23ms/step - loss: 0.6370 - val_loss: 0.6397\n",
      "Epoch 592/1000\n",
      "72/72 [==============================] - 2s 23ms/step - loss: 0.6386 - val_loss: 0.6378\n",
      "Epoch 593/1000\n",
      "72/72 [==============================] - 2s 24ms/step - loss: 0.6380 - val_loss: 0.6379\n",
      "Epoch 594/1000\n",
      "72/72 [==============================] - 2s 25ms/step - loss: 0.6379 - val_loss: 0.6379\n",
      "Epoch 595/1000\n",
      "72/72 [==============================] - 2s 23ms/step - loss: 0.6377 - val_loss: 0.6379\n",
      "Epoch 596/1000\n",
      "72/72 [==============================] - 2s 23ms/step - loss: 0.6369 - val_loss: 0.6362\n",
      "Epoch 597/1000\n",
      "72/72 [==============================] - 2s 24ms/step - loss: 0.6373 - val_loss: 0.6355\n",
      "Epoch 598/1000\n",
      "72/72 [==============================] - 2s 24ms/step - loss: 0.6350 - val_loss: 0.6347\n",
      "Epoch 599/1000\n",
      "72/72 [==============================] - 2s 24ms/step - loss: 0.6360 - val_loss: 0.6349\n",
      "Epoch 600/1000\n",
      "72/72 [==============================] - 2s 24ms/step - loss: 0.6358 - val_loss: 0.6392\n",
      "Epoch 601/1000\n",
      "72/72 [==============================] - 2s 25ms/step - loss: 0.6375 - val_loss: 0.6358\n",
      "Epoch 602/1000\n",
      "72/72 [==============================] - 2s 26ms/step - loss: 0.6352 - val_loss: 0.6343\n",
      "Epoch 603/1000\n",
      "72/72 [==============================] - 2s 26ms/step - loss: 0.6345 - val_loss: 0.6343\n",
      "Epoch 604/1000\n",
      "72/72 [==============================] - 2s 26ms/step - loss: 0.6347 - val_loss: 0.6356\n",
      "Epoch 605/1000\n",
      "72/72 [==============================] - 2s 25ms/step - loss: 0.6343 - val_loss: 0.6339\n",
      "Epoch 606/1000\n",
      "72/72 [==============================] - 2s 26ms/step - loss: 0.6347 - val_loss: 0.6351\n",
      "Epoch 607/1000\n",
      "72/72 [==============================] - 2s 25ms/step - loss: 0.6346 - val_loss: 0.6343\n",
      "Epoch 608/1000\n",
      "72/72 [==============================] - 2s 23ms/step - loss: 0.6342 - val_loss: 0.6342\n",
      "Epoch 609/1000\n",
      "72/72 [==============================] - 2s 25ms/step - loss: 0.6342 - val_loss: 0.6350\n",
      "Epoch 610/1000\n",
      "72/72 [==============================] - 2s 25ms/step - loss: 0.6343 - val_loss: 0.6340\n",
      "Epoch 611/1000\n",
      "72/72 [==============================] - 2s 24ms/step - loss: 0.6347 - val_loss: 0.6347\n",
      "Epoch 612/1000\n",
      "72/72 [==============================] - 2s 23ms/step - loss: 0.6339 - val_loss: 0.6335\n",
      "Epoch 613/1000\n",
      "72/72 [==============================] - 2s 25ms/step - loss: 0.6337 - val_loss: 0.6339\n",
      "Epoch 614/1000\n",
      "72/72 [==============================] - 2s 25ms/step - loss: 0.6340 - val_loss: 0.6341\n",
      "Epoch 615/1000\n",
      "72/72 [==============================] - 2s 25ms/step - loss: 0.6340 - val_loss: 0.6343\n",
      "Epoch 616/1000\n",
      "72/72 [==============================] - 2s 23ms/step - loss: 0.6350 - val_loss: 0.6349\n",
      "Epoch 617/1000\n",
      "72/72 [==============================] - 2s 24ms/step - loss: 0.6360 - val_loss: 0.6358\n",
      "Epoch 618/1000\n",
      "72/72 [==============================] - 2s 24ms/step - loss: 0.6362 - val_loss: 0.6365\n",
      "Epoch 619/1000\n",
      "72/72 [==============================] - 2s 24ms/step - loss: 0.6363 - val_loss: 0.6367\n",
      "Epoch 620/1000\n",
      "72/72 [==============================] - 2s 23ms/step - loss: 0.6373 - val_loss: 0.6371\n",
      "Epoch 621/1000\n",
      "72/72 [==============================] - 2s 23ms/step - loss: 0.6366 - val_loss: 0.6368\n",
      "Epoch 622/1000\n",
      "72/72 [==============================] - 2s 23ms/step - loss: 0.6369 - val_loss: 0.6369\n",
      "Epoch 623/1000\n",
      "72/72 [==============================] - 2s 23ms/step - loss: 0.6372 - val_loss: 0.6367\n",
      "Epoch 624/1000\n",
      "72/72 [==============================] - 2s 23ms/step - loss: 0.6378 - val_loss: 0.6389\n",
      "Epoch 625/1000\n",
      "72/72 [==============================] - 2s 23ms/step - loss: 0.6387 - val_loss: 0.6384\n",
      "Epoch 626/1000\n",
      "72/72 [==============================] - 2s 22ms/step - loss: 0.6377 - val_loss: 0.6377\n",
      "Epoch 627/1000\n",
      "72/72 [==============================] - 2s 24ms/step - loss: 0.6374 - val_loss: 0.6378\n",
      "Epoch 628/1000\n",
      "72/72 [==============================] - 2s 24ms/step - loss: 0.6383 - val_loss: 0.6380\n",
      "Epoch 629/1000\n",
      "72/72 [==============================] - 2s 24ms/step - loss: 0.6384 - val_loss: 0.6384\n",
      "Epoch 630/1000\n",
      "72/72 [==============================] - 2s 23ms/step - loss: 0.6396 - val_loss: 0.6389\n",
      "Epoch 631/1000\n",
      "72/72 [==============================] - 2s 23ms/step - loss: 0.6388 - val_loss: 0.6380\n",
      "Epoch 632/1000\n",
      "72/72 [==============================] - 2s 24ms/step - loss: 0.6382 - val_loss: 0.6378\n",
      "Epoch 633/1000\n",
      "72/72 [==============================] - 2s 23ms/step - loss: 0.6379 - val_loss: 0.6377\n",
      "Epoch 634/1000\n",
      "72/72 [==============================] - 2s 23ms/step - loss: 0.6384 - val_loss: 0.6390\n",
      "Epoch 635/1000\n",
      "72/72 [==============================] - 2s 23ms/step - loss: 0.6392 - val_loss: 0.6386\n",
      "Epoch 636/1000\n",
      "72/72 [==============================] - 2s 24ms/step - loss: 0.6381 - val_loss: 0.6379\n",
      "Epoch 637/1000\n",
      "72/72 [==============================] - 2s 23ms/step - loss: 0.6378 - val_loss: 0.6377\n",
      "Epoch 638/1000\n",
      "72/72 [==============================] - 2s 24ms/step - loss: 0.6378 - val_loss: 0.6376\n",
      "Epoch 639/1000\n",
      "72/72 [==============================] - 2s 24ms/step - loss: 0.6378 - val_loss: 0.6381\n",
      "Epoch 640/1000\n",
      "72/72 [==============================] - 2s 23ms/step - loss: 0.6380 - val_loss: 0.6379\n",
      "Epoch 641/1000\n",
      "72/72 [==============================] - 2s 23ms/step - loss: 0.6377 - val_loss: 0.6379\n",
      "Epoch 642/1000\n",
      "72/72 [==============================] - 2s 24ms/step - loss: 0.6378 - val_loss: 0.6377\n",
      "Epoch 643/1000\n",
      "72/72 [==============================] - 2s 24ms/step - loss: 0.6377 - val_loss: 0.6375\n",
      "Epoch 644/1000\n",
      "72/72 [==============================] - 2s 24ms/step - loss: 0.6375 - val_loss: 0.6375\n",
      "Epoch 645/1000\n",
      "72/72 [==============================] - 2s 24ms/step - loss: 0.6378 - val_loss: 0.6376\n",
      "Epoch 646/1000\n",
      "72/72 [==============================] - 2s 25ms/step - loss: 0.6375 - val_loss: 0.6374\n",
      "Epoch 647/1000\n",
      "72/72 [==============================] - 2s 26ms/step - loss: 0.6379 - val_loss: 0.6376\n",
      "Epoch 648/1000\n",
      "72/72 [==============================] - 2s 25ms/step - loss: 0.6374 - val_loss: 0.6374\n",
      "Epoch 649/1000\n",
      "72/72 [==============================] - 2s 25ms/step - loss: 0.6374 - val_loss: 0.6373\n",
      "Epoch 650/1000\n",
      "72/72 [==============================] - 2s 24ms/step - loss: 0.6374 - val_loss: 0.6373\n",
      "Epoch 651/1000\n",
      "72/72 [==============================] - 2s 24ms/step - loss: 0.6375 - val_loss: 0.6374\n",
      "Epoch 652/1000\n",
      "72/72 [==============================] - 2s 23ms/step - loss: 0.6374 - val_loss: 0.6374\n",
      "Epoch 653/1000\n",
      "72/72 [==============================] - 2s 24ms/step - loss: 0.6374 - val_loss: 0.6374\n",
      "Epoch 654/1000\n",
      "72/72 [==============================] - 2s 23ms/step - loss: 0.6373 - val_loss: 0.6373\n",
      "Epoch 655/1000\n",
      "72/72 [==============================] - 2s 24ms/step - loss: 0.6373 - val_loss: 0.6372\n",
      "Epoch 656/1000\n",
      "72/72 [==============================] - 2s 23ms/step - loss: 0.6373 - val_loss: 0.6372\n",
      "Epoch 657/1000\n",
      "72/72 [==============================] - 2s 23ms/step - loss: 0.6374 - val_loss: 0.6373\n",
      "Epoch 658/1000\n",
      "72/72 [==============================] - 2s 24ms/step - loss: 0.6373 - val_loss: 0.6373\n",
      "Epoch 659/1000\n",
      "72/72 [==============================] - 2s 23ms/step - loss: 0.6373 - val_loss: 0.6375\n",
      "Epoch 660/1000\n",
      "72/72 [==============================] - 2s 24ms/step - loss: 0.6373 - val_loss: 0.6374\n",
      "Epoch 661/1000\n",
      "72/72 [==============================] - 2s 23ms/step - loss: 0.6373 - val_loss: 0.6373\n",
      "Epoch 662/1000\n",
      "72/72 [==============================] - 2s 24ms/step - loss: 0.6373 - val_loss: 0.6387\n",
      "Epoch 663/1000\n",
      "72/72 [==============================] - 2s 25ms/step - loss: 0.6374 - val_loss: 0.6373\n",
      "Epoch 664/1000\n",
      "72/72 [==============================] - 2s 24ms/step - loss: 0.6373 - val_loss: 0.6377\n",
      "Epoch 665/1000\n",
      "72/72 [==============================] - 2s 23ms/step - loss: 0.6374 - val_loss: 0.6373\n",
      "Epoch 666/1000\n",
      "72/72 [==============================] - 2s 23ms/step - loss: 0.6373 - val_loss: 0.6373\n",
      "Epoch 667/1000\n",
      "72/72 [==============================] - 2s 23ms/step - loss: 0.6373 - val_loss: 0.6373\n",
      "Epoch 668/1000\n",
      "72/72 [==============================] - 2s 23ms/step - loss: 0.6372 - val_loss: 0.6372\n",
      "Epoch 669/1000\n",
      "72/72 [==============================] - 2s 23ms/step - loss: 0.6373 - val_loss: 0.6372\n",
      "Epoch 670/1000\n",
      "72/72 [==============================] - 2s 23ms/step - loss: 0.6372 - val_loss: 0.6372\n",
      "Epoch 671/1000\n",
      "72/72 [==============================] - 2s 23ms/step - loss: 0.6373 - val_loss: 0.6372\n",
      "Epoch 672/1000\n",
      "72/72 [==============================] - 2s 25ms/step - loss: 0.6374 - val_loss: 0.6371\n",
      "Epoch 673/1000\n",
      "72/72 [==============================] - 2s 23ms/step - loss: 0.6372 - val_loss: 0.6374\n",
      "Epoch 674/1000\n",
      "72/72 [==============================] - 2s 23ms/step - loss: 0.6374 - val_loss: 0.6373\n",
      "Epoch 675/1000\n",
      "72/72 [==============================] - 2s 27ms/step - loss: 0.6372 - val_loss: 0.6373\n",
      "Epoch 676/1000\n",
      "72/72 [==============================] - 2s 26ms/step - loss: 0.6371 - val_loss: 0.6371\n",
      "Epoch 677/1000\n",
      "72/72 [==============================] - 2s 26ms/step - loss: 0.6372 - val_loss: 0.6372\n",
      "Epoch 678/1000\n",
      "72/72 [==============================] - 2s 25ms/step - loss: 0.6373 - val_loss: 0.6371\n",
      "Epoch 679/1000\n",
      "72/72 [==============================] - 2s 24ms/step - loss: 0.6372 - val_loss: 0.6370\n",
      "Epoch 680/1000\n",
      "72/72 [==============================] - 2s 24ms/step - loss: 0.6371 - val_loss: 0.6372\n",
      "Epoch 681/1000\n",
      "72/72 [==============================] - 2s 24ms/step - loss: 0.6371 - val_loss: 0.6370\n",
      "Epoch 682/1000\n",
      "72/72 [==============================] - 2s 24ms/step - loss: 0.6373 - val_loss: 0.6371\n",
      "Epoch 683/1000\n",
      "72/72 [==============================] - 2s 23ms/step - loss: 0.6371 - val_loss: 0.6376\n",
      "Epoch 684/1000\n",
      "72/72 [==============================] - 2s 22ms/step - loss: 0.6373 - val_loss: 0.6373\n",
      "Epoch 685/1000\n",
      "72/72 [==============================] - 2s 24ms/step - loss: 0.6379 - val_loss: 0.6373\n",
      "Epoch 686/1000\n",
      "72/72 [==============================] - 2s 23ms/step - loss: 0.6373 - val_loss: 0.6371\n",
      "Epoch 687/1000\n",
      "72/72 [==============================] - 2s 23ms/step - loss: 0.6372 - val_loss: 0.6371\n",
      "Epoch 688/1000\n",
      "72/72 [==============================] - 2s 24ms/step - loss: 0.6371 - val_loss: 0.6371\n",
      "Epoch 689/1000\n",
      "72/72 [==============================] - 2s 25ms/step - loss: 0.6371 - val_loss: 0.6372\n",
      "Epoch 690/1000\n",
      "72/72 [==============================] - 2s 25ms/step - loss: 0.6372 - val_loss: 0.6371\n",
      "Epoch 691/1000\n",
      "72/72 [==============================] - 2s 25ms/step - loss: 0.6371 - val_loss: 0.6371\n",
      "Epoch 692/1000\n",
      "72/72 [==============================] - 2s 23ms/step - loss: 0.6374 - val_loss: 0.6371\n",
      "Epoch 693/1000\n",
      "72/72 [==============================] - 2s 24ms/step - loss: 0.6371 - val_loss: 0.6371\n",
      "Epoch 694/1000\n",
      "72/72 [==============================] - 2s 23ms/step - loss: 0.6371 - val_loss: 0.6370\n",
      "Epoch 695/1000\n",
      "72/72 [==============================] - 2s 24ms/step - loss: 0.6370 - val_loss: 0.6370\n",
      "Epoch 696/1000\n",
      "72/72 [==============================] - 2s 26ms/step - loss: 0.6371 - val_loss: 0.6373\n",
      "Epoch 697/1000\n",
      "72/72 [==============================] - 2s 25ms/step - loss: 0.6373 - val_loss: 0.6374\n",
      "Epoch 698/1000\n",
      "72/72 [==============================] - 2s 23ms/step - loss: 0.6371 - val_loss: 0.6370\n",
      "Epoch 699/1000\n",
      "72/72 [==============================] - 2s 24ms/step - loss: 0.6371 - val_loss: 0.6369\n",
      "Epoch 700/1000\n",
      "72/72 [==============================] - 2s 24ms/step - loss: 0.6371 - val_loss: 0.6370\n",
      "Epoch 701/1000\n",
      "72/72 [==============================] - 2s 24ms/step - loss: 0.6371 - val_loss: 0.6370\n",
      "Epoch 702/1000\n",
      "72/72 [==============================] - 2s 23ms/step - loss: 0.6372 - val_loss: 0.6371\n",
      "Epoch 703/1000\n",
      "72/72 [==============================] - 2s 23ms/step - loss: 0.6374 - val_loss: 0.6370\n",
      "Epoch 704/1000\n",
      "72/72 [==============================] - 2s 23ms/step - loss: 0.6370 - val_loss: 0.6370\n",
      "Epoch 705/1000\n",
      "72/72 [==============================] - 2s 24ms/step - loss: 0.6370 - val_loss: 0.6370\n",
      "Epoch 706/1000\n",
      "72/72 [==============================] - 2s 24ms/step - loss: 0.6371 - val_loss: 0.6377\n",
      "Epoch 707/1000\n",
      "72/72 [==============================] - 2s 24ms/step - loss: 0.6371 - val_loss: 0.6370\n",
      "Epoch 708/1000\n",
      "72/72 [==============================] - 2s 27ms/step - loss: 0.6370 - val_loss: 0.6371\n",
      "Epoch 709/1000\n",
      "72/72 [==============================] - 2s 26ms/step - loss: 0.6370 - val_loss: 0.6369\n",
      "Epoch 710/1000\n",
      "72/72 [==============================] - 2s 23ms/step - loss: 0.6370 - val_loss: 0.6369\n",
      "Epoch 711/1000\n",
      "72/72 [==============================] - 2s 24ms/step - loss: 0.6371 - val_loss: 0.6369\n",
      "Epoch 712/1000\n",
      "72/72 [==============================] - 2s 25ms/step - loss: 0.6370 - val_loss: 0.6369\n",
      "Epoch 713/1000\n",
      "72/72 [==============================] - 2s 25ms/step - loss: 0.6369 - val_loss: 0.6371\n",
      "Epoch 714/1000\n",
      "72/72 [==============================] - 2s 24ms/step - loss: 0.6370 - val_loss: 0.6370\n",
      "Epoch 715/1000\n",
      "72/72 [==============================] - 2s 24ms/step - loss: 0.6370 - val_loss: 0.6369\n",
      "Epoch 716/1000\n",
      "72/72 [==============================] - 2s 23ms/step - loss: 0.6369 - val_loss: 0.6369\n",
      "Epoch 717/1000\n",
      "72/72 [==============================] - 2s 26ms/step - loss: 0.6369 - val_loss: 0.6370\n",
      "Epoch 718/1000\n",
      "72/72 [==============================] - 2s 25ms/step - loss: 0.6369 - val_loss: 0.6369\n",
      "Epoch 719/1000\n",
      "72/72 [==============================] - 2s 24ms/step - loss: 0.6370 - val_loss: 0.6370\n",
      "Epoch 720/1000\n",
      "72/72 [==============================] - 2s 23ms/step - loss: 0.6370 - val_loss: 0.6368\n",
      "Epoch 721/1000\n",
      "72/72 [==============================] - 2s 23ms/step - loss: 0.6369 - val_loss: 0.6370\n",
      "Epoch 722/1000\n",
      "72/72 [==============================] - 2s 24ms/step - loss: 0.6370 - val_loss: 0.6368\n",
      "Epoch 723/1000\n",
      "72/72 [==============================] - 2s 24ms/step - loss: 0.6369 - val_loss: 0.6368\n",
      "Epoch 724/1000\n",
      "72/72 [==============================] - 2s 25ms/step - loss: 0.6368 - val_loss: 0.6368\n",
      "Epoch 725/1000\n",
      "72/72 [==============================] - 2s 25ms/step - loss: 0.6369 - val_loss: 0.6369\n",
      "Epoch 726/1000\n",
      "72/72 [==============================] - 2s 24ms/step - loss: 0.6370 - val_loss: 0.6369\n",
      "Epoch 727/1000\n",
      "72/72 [==============================] - 2s 24ms/step - loss: 0.6369 - val_loss: 0.6367\n",
      "Epoch 728/1000\n",
      "72/72 [==============================] - 2s 26ms/step - loss: 0.6368 - val_loss: 0.6367\n",
      "Epoch 729/1000\n",
      "72/72 [==============================] - 2s 25ms/step - loss: 0.6368 - val_loss: 0.6368\n",
      "Epoch 730/1000\n",
      "72/72 [==============================] - 2s 23ms/step - loss: 0.6369 - val_loss: 0.6368\n",
      "Epoch 731/1000\n",
      "72/72 [==============================] - 2s 24ms/step - loss: 0.6368 - val_loss: 0.6367\n",
      "Epoch 732/1000\n",
      "72/72 [==============================] - 2s 24ms/step - loss: 0.6368 - val_loss: 0.6367\n",
      "Epoch 733/1000\n",
      "72/72 [==============================] - 2s 24ms/step - loss: 0.6368 - val_loss: 0.6366\n",
      "Epoch 734/1000\n",
      "72/72 [==============================] - 2s 24ms/step - loss: 0.6367 - val_loss: 0.6366\n",
      "Epoch 735/1000\n",
      "72/72 [==============================] - 2s 23ms/step - loss: 0.6368 - val_loss: 0.6367\n",
      "Epoch 736/1000\n",
      "72/72 [==============================] - 2s 24ms/step - loss: 0.6367 - val_loss: 0.6367\n",
      "Epoch 737/1000\n",
      "72/72 [==============================] - 2s 24ms/step - loss: 0.6367 - val_loss: 0.6367\n",
      "Epoch 738/1000\n",
      "72/72 [==============================] - 2s 26ms/step - loss: 0.6367 - val_loss: 0.6367\n",
      "Epoch 739/1000\n",
      "72/72 [==============================] - 2s 25ms/step - loss: 0.6366 - val_loss: 0.6365\n",
      "Epoch 740/1000\n",
      "72/72 [==============================] - 2s 25ms/step - loss: 0.6367 - val_loss: 0.6367\n",
      "Epoch 741/1000\n",
      "72/72 [==============================] - 2s 24ms/step - loss: 0.6366 - val_loss: 0.6369\n",
      "Epoch 742/1000\n",
      "72/72 [==============================] - 2s 24ms/step - loss: 0.6366 - val_loss: 0.6365\n",
      "Epoch 743/1000\n",
      "72/72 [==============================] - 2s 23ms/step - loss: 0.6366 - val_loss: 0.6364\n",
      "Epoch 744/1000\n",
      "72/72 [==============================] - 2s 24ms/step - loss: 0.6366 - val_loss: 0.6364\n",
      "Epoch 745/1000\n",
      "72/72 [==============================] - 2s 23ms/step - loss: 0.6366 - val_loss: 0.6363\n",
      "Epoch 746/1000\n",
      "72/72 [==============================] - 2s 25ms/step - loss: 0.6364 - val_loss: 0.6363\n",
      "Epoch 747/1000\n",
      "72/72 [==============================] - 2s 25ms/step - loss: 0.6364 - val_loss: 0.6362\n",
      "Epoch 748/1000\n",
      "72/72 [==============================] - 2s 24ms/step - loss: 0.6365 - val_loss: 0.6363\n",
      "Epoch 749/1000\n",
      "72/72 [==============================] - 2s 24ms/step - loss: 0.6363 - val_loss: 0.6361\n",
      "Epoch 750/1000\n",
      "72/72 [==============================] - 2s 24ms/step - loss: 0.6363 - val_loss: 0.6361\n",
      "Epoch 751/1000\n",
      "72/72 [==============================] - 2s 24ms/step - loss: 0.6363 - val_loss: 0.6362\n",
      "Epoch 752/1000\n",
      "72/72 [==============================] - 2s 23ms/step - loss: 0.6363 - val_loss: 0.6359\n",
      "Epoch 753/1000\n",
      "72/72 [==============================] - 2s 24ms/step - loss: 0.6361 - val_loss: 0.6360\n",
      "Epoch 754/1000\n",
      "72/72 [==============================] - 2s 23ms/step - loss: 0.6362 - val_loss: 0.6361\n",
      "Epoch 755/1000\n",
      "72/72 [==============================] - 2s 24ms/step - loss: 0.6360 - val_loss: 0.6359\n",
      "Epoch 756/1000\n",
      "72/72 [==============================] - 2s 24ms/step - loss: 0.6360 - val_loss: 0.6364\n",
      "Epoch 757/1000\n",
      "72/72 [==============================] - 2s 24ms/step - loss: 0.6361 - val_loss: 0.6359\n",
      "Epoch 758/1000\n",
      "72/72 [==============================] - 2s 24ms/step - loss: 0.6361 - val_loss: 0.6359\n",
      "Epoch 759/1000\n",
      "72/72 [==============================] - 2s 23ms/step - loss: 0.6364 - val_loss: 0.6363\n",
      "Epoch 760/1000\n",
      "72/72 [==============================] - 2s 24ms/step - loss: 0.6361 - val_loss: 0.6358\n",
      "Epoch 761/1000\n",
      "72/72 [==============================] - 2s 23ms/step - loss: 0.6359 - val_loss: 0.6356\n",
      "Epoch 762/1000\n",
      "72/72 [==============================] - 2s 24ms/step - loss: 0.6358 - val_loss: 0.6358\n",
      "Epoch 763/1000\n",
      "72/72 [==============================] - 2s 23ms/step - loss: 0.6358 - val_loss: 0.6356\n",
      "Epoch 764/1000\n",
      "72/72 [==============================] - 2s 24ms/step - loss: 0.6361 - val_loss: 0.6359\n",
      "Epoch 765/1000\n",
      "72/72 [==============================] - 2s 24ms/step - loss: 0.6359 - val_loss: 0.6359\n",
      "Epoch 766/1000\n",
      "72/72 [==============================] - 2s 24ms/step - loss: 0.6359 - val_loss: 0.6357\n",
      "Epoch 767/1000\n",
      "72/72 [==============================] - 2s 23ms/step - loss: 0.6358 - val_loss: 0.6357\n",
      "Epoch 768/1000\n",
      "72/72 [==============================] - 2s 23ms/step - loss: 0.6357 - val_loss: 0.6356\n",
      "Epoch 769/1000\n",
      "72/72 [==============================] - 2s 24ms/step - loss: 0.6357 - val_loss: 0.6356\n",
      "Epoch 770/1000\n",
      "72/72 [==============================] - 2s 24ms/step - loss: 0.6357 - val_loss: 0.6357\n",
      "Epoch 771/1000\n",
      "72/72 [==============================] - 2s 24ms/step - loss: 0.6358 - val_loss: 0.6357\n",
      "Epoch 772/1000\n",
      "72/72 [==============================] - 2s 23ms/step - loss: 0.6357 - val_loss: 0.6358\n",
      "Epoch 773/1000\n",
      "72/72 [==============================] - 2s 23ms/step - loss: 0.6358 - val_loss: 0.6361\n",
      "Epoch 774/1000\n",
      "72/72 [==============================] - 2s 23ms/step - loss: 0.6357 - val_loss: 0.6357\n",
      "Epoch 775/1000\n",
      "72/72 [==============================] - 2s 24ms/step - loss: 0.6354 - val_loss: 0.6357\n",
      "Epoch 776/1000\n",
      "72/72 [==============================] - 2s 23ms/step - loss: 0.6356 - val_loss: 0.6352\n",
      "Epoch 777/1000\n",
      "72/72 [==============================] - 2s 24ms/step - loss: 0.6355 - val_loss: 0.6355\n",
      "Epoch 778/1000\n",
      "72/72 [==============================] - 2s 24ms/step - loss: 0.6355 - val_loss: 0.6358\n",
      "Epoch 779/1000\n",
      "72/72 [==============================] - 2s 25ms/step - loss: 0.6357 - val_loss: 0.6354\n",
      "Epoch 780/1000\n",
      "72/72 [==============================] - 2s 24ms/step - loss: 0.6354 - val_loss: 0.6354\n",
      "Epoch 781/1000\n",
      "72/72 [==============================] - 2s 24ms/step - loss: 0.6354 - val_loss: 0.6355\n",
      "Epoch 782/1000\n",
      "72/72 [==============================] - 2s 23ms/step - loss: 0.6354 - val_loss: 0.6352\n",
      "Epoch 783/1000\n",
      "72/72 [==============================] - 2s 24ms/step - loss: 0.6353 - val_loss: 0.6351\n",
      "Epoch 784/1000\n",
      "72/72 [==============================] - 2s 24ms/step - loss: 0.6352 - val_loss: 0.6351\n",
      "Epoch 785/1000\n",
      "72/72 [==============================] - 2s 24ms/step - loss: 0.6355 - val_loss: 0.6360\n",
      "Epoch 786/1000\n",
      "72/72 [==============================] - 2s 23ms/step - loss: 0.6358 - val_loss: 0.6357\n",
      "Epoch 787/1000\n",
      "72/72 [==============================] - 2s 24ms/step - loss: 0.6353 - val_loss: 0.6351\n",
      "Epoch 788/1000\n",
      "72/72 [==============================] - 2s 24ms/step - loss: 0.6356 - val_loss: 0.6354\n",
      "Epoch 789/1000\n",
      "72/72 [==============================] - 2s 23ms/step - loss: 0.6352 - val_loss: 0.6356\n",
      "Epoch 790/1000\n",
      "72/72 [==============================] - 2s 24ms/step - loss: 0.6351 - val_loss: 0.6349\n",
      "Epoch 791/1000\n",
      "72/72 [==============================] - 2s 22ms/step - loss: 0.6349 - val_loss: 0.6351\n",
      "Epoch 792/1000\n",
      "72/72 [==============================] - 1s 21ms/step - loss: 0.6351 - val_loss: 0.6350\n",
      "Epoch 793/1000\n",
      "72/72 [==============================] - 1s 20ms/step - loss: 0.6351 - val_loss: 0.6358\n",
      "Epoch 794/1000\n",
      "72/72 [==============================] - 2s 21ms/step - loss: 0.6351 - val_loss: 0.6351\n",
      "Epoch 795/1000\n",
      "72/72 [==============================] - 1s 21ms/step - loss: 0.6351 - val_loss: 0.6351\n",
      "Epoch 796/1000\n",
      "72/72 [==============================] - 1s 21ms/step - loss: 0.6350 - val_loss: 0.6352\n",
      "Epoch 797/1000\n",
      "72/72 [==============================] - 1s 21ms/step - loss: 0.6355 - val_loss: 0.6357\n",
      "Epoch 798/1000\n",
      "72/72 [==============================] - 1s 20ms/step - loss: 0.6352 - val_loss: 0.6357\n",
      "Epoch 799/1000\n",
      "72/72 [==============================] - 2s 21ms/step - loss: 0.6351 - val_loss: 0.6347\n",
      "Epoch 800/1000\n",
      "72/72 [==============================] - 2s 22ms/step - loss: 0.6355 - val_loss: 0.6355\n",
      "Epoch 801/1000\n",
      "72/72 [==============================] - 1s 21ms/step - loss: 0.6349 - val_loss: 0.6351\n",
      "Epoch 802/1000\n",
      "72/72 [==============================] - 2s 21ms/step - loss: 0.6349 - val_loss: 0.6346\n",
      "Epoch 803/1000\n",
      "72/72 [==============================] - 2s 22ms/step - loss: 0.6348 - val_loss: 0.6349\n",
      "Epoch 804/1000\n",
      "72/72 [==============================] - 2s 23ms/step - loss: 0.6348 - val_loss: 0.6346\n",
      "Epoch 805/1000\n",
      "72/72 [==============================] - 2s 23ms/step - loss: 0.6348 - val_loss: 0.6347\n",
      "Epoch 806/1000\n",
      "72/72 [==============================] - 2s 23ms/step - loss: 0.6347 - val_loss: 0.6348\n",
      "Epoch 807/1000\n",
      "72/72 [==============================] - 2s 23ms/step - loss: 0.6351 - val_loss: 0.6350\n",
      "Epoch 808/1000\n",
      "72/72 [==============================] - 2s 23ms/step - loss: 0.6347 - val_loss: 0.6345\n",
      "Epoch 809/1000\n",
      "72/72 [==============================] - 2s 23ms/step - loss: 0.6346 - val_loss: 0.6344\n",
      "Epoch 810/1000\n",
      "72/72 [==============================] - 2s 23ms/step - loss: 0.6345 - val_loss: 0.6345\n",
      "Epoch 811/1000\n",
      "72/72 [==============================] - 2s 23ms/step - loss: 0.6346 - val_loss: 0.6347\n",
      "Epoch 812/1000\n",
      "72/72 [==============================] - 2s 25ms/step - loss: 0.6348 - val_loss: 0.6346\n",
      "Epoch 813/1000\n",
      "72/72 [==============================] - 2s 23ms/step - loss: 0.6353 - val_loss: 0.6345\n",
      "Epoch 814/1000\n",
      "72/72 [==============================] - 2s 24ms/step - loss: 0.6347 - val_loss: 0.6349\n",
      "Epoch 815/1000\n",
      "72/72 [==============================] - 2s 24ms/step - loss: 0.6347 - val_loss: 0.6351\n",
      "Epoch 816/1000\n",
      "72/72 [==============================] - 2s 23ms/step - loss: 0.6349 - val_loss: 0.6345\n",
      "Epoch 817/1000\n",
      "72/72 [==============================] - 2s 23ms/step - loss: 0.6349 - val_loss: 0.6349\n",
      "Epoch 818/1000\n",
      "72/72 [==============================] - 2s 23ms/step - loss: 0.6347 - val_loss: 0.6346\n",
      "Epoch 819/1000\n",
      "72/72 [==============================] - 2s 23ms/step - loss: 0.6346 - val_loss: 0.6347\n",
      "Epoch 820/1000\n",
      "72/72 [==============================] - 2s 23ms/step - loss: 0.6343 - val_loss: 0.6342\n",
      "Epoch 821/1000\n",
      "72/72 [==============================] - 2s 23ms/step - loss: 0.6346 - val_loss: 0.6343\n",
      "Epoch 822/1000\n",
      "72/72 [==============================] - 2s 24ms/step - loss: 0.6348 - val_loss: 0.6343\n",
      "Epoch 823/1000\n",
      "72/72 [==============================] - 2s 26ms/step - loss: 0.6344 - val_loss: 0.6341\n",
      "Epoch 824/1000\n",
      "72/72 [==============================] - 2s 28ms/step - loss: 0.6342 - val_loss: 0.6344\n",
      "Epoch 825/1000\n",
      "72/72 [==============================] - 2s 25ms/step - loss: 0.6344 - val_loss: 0.6343\n",
      "Epoch 826/1000\n",
      "72/72 [==============================] - 2s 25ms/step - loss: 0.6345 - val_loss: 0.6341\n",
      "Epoch 827/1000\n",
      "72/72 [==============================] - 2s 24ms/step - loss: 0.6343 - val_loss: 0.6341\n",
      "Epoch 828/1000\n",
      "72/72 [==============================] - 2s 25ms/step - loss: 0.6341 - val_loss: 0.6339\n",
      "Epoch 829/1000\n",
      "72/72 [==============================] - 2s 25ms/step - loss: 0.6341 - val_loss: 0.6341\n",
      "Epoch 830/1000\n",
      "72/72 [==============================] - 2s 26ms/step - loss: 0.6340 - val_loss: 0.6341\n",
      "Epoch 831/1000\n",
      "72/72 [==============================] - 2s 26ms/step - loss: 0.6340 - val_loss: 0.6337\n",
      "Epoch 832/1000\n",
      "72/72 [==============================] - 2s 24ms/step - loss: 0.6341 - val_loss: 0.6338\n",
      "Epoch 833/1000\n",
      "72/72 [==============================] - 2s 24ms/step - loss: 0.6344 - val_loss: 0.6361\n",
      "Epoch 834/1000\n",
      "72/72 [==============================] - 2s 23ms/step - loss: 0.6352 - val_loss: 0.6352\n",
      "Epoch 835/1000\n",
      "72/72 [==============================] - 2s 24ms/step - loss: 0.6350 - val_loss: 0.6345\n",
      "Epoch 836/1000\n",
      "72/72 [==============================] - 2s 24ms/step - loss: 0.6345 - val_loss: 0.6342\n",
      "Epoch 837/1000\n",
      "72/72 [==============================] - 2s 24ms/step - loss: 0.6344 - val_loss: 0.6342\n",
      "Epoch 838/1000\n",
      "72/72 [==============================] - 2s 24ms/step - loss: 0.6344 - val_loss: 0.6340\n",
      "Epoch 839/1000\n",
      "72/72 [==============================] - 2s 24ms/step - loss: 0.6346 - val_loss: 0.6354\n",
      "Epoch 840/1000\n",
      "72/72 [==============================] - 2s 24ms/step - loss: 0.6348 - val_loss: 0.6346\n",
      "Epoch 841/1000\n",
      "72/72 [==============================] - 2s 24ms/step - loss: 0.6348 - val_loss: 0.6345\n",
      "Epoch 842/1000\n",
      "72/72 [==============================] - 2s 24ms/step - loss: 0.6346 - val_loss: 0.6347\n",
      "Epoch 843/1000\n",
      "72/72 [==============================] - 2s 24ms/step - loss: 0.6343 - val_loss: 0.6342\n",
      "Epoch 844/1000\n",
      "72/72 [==============================] - 2s 24ms/step - loss: 0.6342 - val_loss: 0.6339\n",
      "Epoch 845/1000\n",
      "72/72 [==============================] - 2s 24ms/step - loss: 0.6340 - val_loss: 0.6344\n",
      "Epoch 846/1000\n",
      "72/72 [==============================] - 2s 22ms/step - loss: 0.6342 - val_loss: 0.6338\n",
      "Epoch 847/1000\n",
      "72/72 [==============================] - 2s 23ms/step - loss: 0.6339 - val_loss: 0.6341\n",
      "Epoch 848/1000\n",
      "72/72 [==============================] - 2s 23ms/step - loss: 0.6339 - val_loss: 0.6338\n",
      "Epoch 849/1000\n",
      "72/72 [==============================] - 2s 23ms/step - loss: 0.6340 - val_loss: 0.6343\n",
      "Epoch 850/1000\n",
      "72/72 [==============================] - 2s 23ms/step - loss: 0.6347 - val_loss: 0.6369\n",
      "Epoch 851/1000\n",
      "72/72 [==============================] - 2s 24ms/step - loss: 0.6381 - val_loss: 0.6384\n",
      "Epoch 852/1000\n",
      "72/72 [==============================] - 2s 23ms/step - loss: 0.6372 - val_loss: 0.6368\n",
      "Epoch 853/1000\n",
      "72/72 [==============================] - 2s 24ms/step - loss: 0.6370 - val_loss: 0.6366\n",
      "Epoch 854/1000\n",
      "72/72 [==============================] - 2s 23ms/step - loss: 0.6358 - val_loss: 0.6347\n",
      "Epoch 855/1000\n",
      "72/72 [==============================] - 2s 23ms/step - loss: 0.6345 - val_loss: 0.6339\n",
      "Epoch 856/1000\n",
      "72/72 [==============================] - 2s 23ms/step - loss: 0.6357 - val_loss: 0.6345\n",
      "Epoch 857/1000\n",
      "72/72 [==============================] - 2s 24ms/step - loss: 0.6338 - val_loss: 0.6350\n",
      "Epoch 858/1000\n",
      "72/72 [==============================] - 2s 24ms/step - loss: 0.6342 - val_loss: 0.6344\n",
      "Epoch 859/1000\n",
      "72/72 [==============================] - 2s 24ms/step - loss: 0.6338 - val_loss: 0.6333\n",
      "Epoch 860/1000\n",
      "72/72 [==============================] - 2s 24ms/step - loss: 0.6336 - val_loss: 0.6335\n",
      "Epoch 861/1000\n",
      "72/72 [==============================] - 2s 23ms/step - loss: 0.6337 - val_loss: 0.6333\n",
      "Epoch 862/1000\n",
      "72/72 [==============================] - 2s 23ms/step - loss: 0.6337 - val_loss: 0.6342\n",
      "Epoch 863/1000\n",
      "72/72 [==============================] - 2s 23ms/step - loss: 0.6343 - val_loss: 0.6338\n",
      "Epoch 864/1000\n",
      "72/72 [==============================] - 2s 23ms/step - loss: 0.6336 - val_loss: 0.6332\n",
      "Epoch 865/1000\n",
      "72/72 [==============================] - 2s 23ms/step - loss: 0.6337 - val_loss: 0.6341\n",
      "Epoch 866/1000\n",
      "72/72 [==============================] - 2s 23ms/step - loss: 0.6335 - val_loss: 0.6332\n",
      "Epoch 867/1000\n",
      "72/72 [==============================] - 2s 23ms/step - loss: 0.6333 - val_loss: 0.6333\n",
      "Epoch 868/1000\n",
      "72/72 [==============================] - 2s 25ms/step - loss: 0.6335 - val_loss: 0.6332\n",
      "Epoch 869/1000\n",
      "72/72 [==============================] - 2s 24ms/step - loss: 0.6332 - val_loss: 0.6332\n",
      "Epoch 870/1000\n",
      "72/72 [==============================] - 2s 24ms/step - loss: 0.6338 - val_loss: 0.6346\n",
      "Epoch 871/1000\n",
      "72/72 [==============================] - 2s 24ms/step - loss: 0.6358 - val_loss: 0.6350\n",
      "Epoch 872/1000\n",
      "72/72 [==============================] - 2s 24ms/step - loss: 0.6345 - val_loss: 0.6340\n",
      "Epoch 873/1000\n",
      "72/72 [==============================] - 2s 23ms/step - loss: 0.6335 - val_loss: 0.6334\n",
      "Epoch 874/1000\n",
      "72/72 [==============================] - 2s 24ms/step - loss: 0.6335 - val_loss: 0.6338\n",
      "Epoch 875/1000\n",
      "72/72 [==============================] - 2s 24ms/step - loss: 0.6335 - val_loss: 0.6333\n",
      "Epoch 876/1000\n",
      "72/72 [==============================] - 2s 23ms/step - loss: 0.6331 - val_loss: 0.6331\n",
      "Epoch 877/1000\n",
      "72/72 [==============================] - 2s 23ms/step - loss: 0.6334 - val_loss: 0.6340\n",
      "Epoch 878/1000\n",
      "72/72 [==============================] - 2s 24ms/step - loss: 0.6337 - val_loss: 0.6339\n",
      "Epoch 879/1000\n",
      "72/72 [==============================] - 2s 23ms/step - loss: 0.6332 - val_loss: 0.6331\n",
      "Epoch 880/1000\n",
      "72/72 [==============================] - 2s 23ms/step - loss: 0.6332 - val_loss: 0.6332\n",
      "Epoch 881/1000\n",
      "72/72 [==============================] - 2s 23ms/step - loss: 0.6332 - val_loss: 0.6340\n",
      "Epoch 882/1000\n",
      "72/72 [==============================] - 2s 23ms/step - loss: 0.6334 - val_loss: 0.6330\n",
      "Epoch 883/1000\n",
      "72/72 [==============================] - 2s 23ms/step - loss: 0.6335 - val_loss: 0.6335\n",
      "Epoch 884/1000\n",
      "72/72 [==============================] - 2s 24ms/step - loss: 0.6337 - val_loss: 0.6348\n",
      "Epoch 885/1000\n",
      "72/72 [==============================] - 2s 23ms/step - loss: 0.6371 - val_loss: 0.6386\n",
      "Epoch 886/1000\n",
      "72/72 [==============================] - 2s 23ms/step - loss: 0.6379 - val_loss: 0.6386\n",
      "Epoch 887/1000\n",
      "72/72 [==============================] - 2s 24ms/step - loss: 0.6375 - val_loss: 0.6363\n",
      "Epoch 888/1000\n",
      "72/72 [==============================] - 2s 26ms/step - loss: 0.6348 - val_loss: 0.6339\n",
      "Epoch 889/1000\n",
      "72/72 [==============================] - 2s 26ms/step - loss: 0.6336 - val_loss: 0.6335\n",
      "Epoch 890/1000\n",
      "72/72 [==============================] - 2s 25ms/step - loss: 0.6344 - val_loss: 0.6354\n",
      "Epoch 891/1000\n",
      "72/72 [==============================] - 2s 24ms/step - loss: 0.6340 - val_loss: 0.6335\n",
      "Epoch 892/1000\n",
      "72/72 [==============================] - 2s 24ms/step - loss: 0.6334 - val_loss: 0.6335\n",
      "Epoch 893/1000\n",
      "72/72 [==============================] - 2s 25ms/step - loss: 0.6335 - val_loss: 0.6337\n",
      "Epoch 894/1000\n",
      "72/72 [==============================] - 2s 24ms/step - loss: 0.6340 - val_loss: 0.6343\n",
      "Epoch 895/1000\n",
      "72/72 [==============================] - 2s 23ms/step - loss: 0.6336 - val_loss: 0.6332\n",
      "Epoch 896/1000\n",
      "72/72 [==============================] - 2s 23ms/step - loss: 0.6333 - val_loss: 0.6331\n",
      "Epoch 897/1000\n",
      "72/72 [==============================] - 2s 23ms/step - loss: 0.6333 - val_loss: 0.6333\n",
      "Epoch 898/1000\n",
      "72/72 [==============================] - 2s 23ms/step - loss: 0.6340 - val_loss: 0.6362\n",
      "Epoch 899/1000\n",
      "72/72 [==============================] - 2s 25ms/step - loss: 0.6349 - val_loss: 0.6339\n",
      "Epoch 900/1000\n",
      "72/72 [==============================] - 2s 24ms/step - loss: 0.6333 - val_loss: 0.6329\n",
      "Epoch 901/1000\n",
      "72/72 [==============================] - 2s 24ms/step - loss: 0.6331 - val_loss: 0.6330\n",
      "Epoch 902/1000\n",
      "72/72 [==============================] - 2s 23ms/step - loss: 0.6329 - val_loss: 0.6330\n",
      "Epoch 903/1000\n",
      "72/72 [==============================] - 2s 22ms/step - loss: 0.6334 - val_loss: 0.6333\n",
      "Epoch 904/1000\n",
      "72/72 [==============================] - 2s 23ms/step - loss: 0.6331 - val_loss: 0.6328\n",
      "Epoch 905/1000\n",
      "72/72 [==============================] - 2s 23ms/step - loss: 0.6328 - val_loss: 0.6327\n",
      "Epoch 906/1000\n",
      "72/72 [==============================] - 2s 23ms/step - loss: 0.6330 - val_loss: 0.6331\n",
      "Epoch 907/1000\n",
      "72/72 [==============================] - 2s 23ms/step - loss: 0.6329 - val_loss: 0.6328\n",
      "Epoch 908/1000\n",
      "72/72 [==============================] - 2s 24ms/step - loss: 0.6329 - val_loss: 0.6327\n",
      "Epoch 909/1000\n",
      "72/72 [==============================] - 2s 24ms/step - loss: 0.6328 - val_loss: 0.6330\n",
      "Epoch 910/1000\n",
      "72/72 [==============================] - 2s 24ms/step - loss: 0.6336 - val_loss: 0.6336\n",
      "Epoch 911/1000\n",
      "72/72 [==============================] - 2s 24ms/step - loss: 0.6339 - val_loss: 0.6345\n",
      "Epoch 912/1000\n",
      "72/72 [==============================] - 2s 23ms/step - loss: 0.6345 - val_loss: 0.6336\n",
      "Epoch 913/1000\n",
      "72/72 [==============================] - 2s 25ms/step - loss: 0.6337 - val_loss: 0.6333\n",
      "Epoch 914/1000\n",
      "72/72 [==============================] - 2s 23ms/step - loss: 0.6339 - val_loss: 0.6341\n",
      "Epoch 915/1000\n",
      "72/72 [==============================] - 2s 23ms/step - loss: 0.6339 - val_loss: 0.6335\n",
      "Epoch 916/1000\n",
      "72/72 [==============================] - 2s 24ms/step - loss: 0.6339 - val_loss: 0.6332\n",
      "Epoch 917/1000\n",
      "72/72 [==============================] - 2s 23ms/step - loss: 0.6332 - val_loss: 0.6333\n",
      "Epoch 918/1000\n",
      "72/72 [==============================] - 2s 24ms/step - loss: 0.6333 - val_loss: 0.6329\n",
      "Epoch 919/1000\n",
      "72/72 [==============================] - 2s 24ms/step - loss: 0.6331 - val_loss: 0.6334\n",
      "Epoch 920/1000\n",
      "72/72 [==============================] - 2s 23ms/step - loss: 0.6333 - val_loss: 0.6326\n",
      "Epoch 921/1000\n",
      "72/72 [==============================] - 2s 24ms/step - loss: 0.6328 - val_loss: 0.6328\n",
      "Epoch 922/1000\n",
      "72/72 [==============================] - 2s 24ms/step - loss: 0.6328 - val_loss: 0.6328\n",
      "Epoch 923/1000\n",
      "72/72 [==============================] - 2s 24ms/step - loss: 0.6328 - val_loss: 0.6326\n",
      "Epoch 924/1000\n",
      "72/72 [==============================] - 2s 24ms/step - loss: 0.6361 - val_loss: 0.6409\n",
      "Epoch 925/1000\n",
      "72/72 [==============================] - 2s 24ms/step - loss: 0.6407 - val_loss: 0.6389\n",
      "Epoch 926/1000\n",
      "72/72 [==============================] - 2s 24ms/step - loss: 0.6384 - val_loss: 0.6380\n",
      "Epoch 927/1000\n",
      "72/72 [==============================] - 2s 23ms/step - loss: 0.6381 - val_loss: 0.6381\n",
      "Epoch 928/1000\n",
      "72/72 [==============================] - 2s 23ms/step - loss: 0.6381 - val_loss: 0.6375\n",
      "Epoch 929/1000\n",
      "72/72 [==============================] - 2s 23ms/step - loss: 0.6385 - val_loss: 0.6380\n",
      "Epoch 930/1000\n",
      "72/72 [==============================] - 2s 24ms/step - loss: 0.6379 - val_loss: 0.6377\n",
      "Epoch 931/1000\n",
      "72/72 [==============================] - 2s 24ms/step - loss: 0.6378 - val_loss: 0.6379\n",
      "Epoch 932/1000\n",
      "72/72 [==============================] - 2s 23ms/step - loss: 0.6377 - val_loss: 0.6377\n",
      "Epoch 933/1000\n",
      "72/72 [==============================] - 2s 23ms/step - loss: 0.6376 - val_loss: 0.6377\n",
      "Epoch 934/1000\n",
      "72/72 [==============================] - 2s 23ms/step - loss: 0.6376 - val_loss: 0.6376\n",
      "Epoch 935/1000\n",
      "72/72 [==============================] - 2s 23ms/step - loss: 0.6376 - val_loss: 0.6377\n",
      "Epoch 936/1000\n",
      "72/72 [==============================] - 2s 23ms/step - loss: 0.6376 - val_loss: 0.6376\n",
      "Epoch 937/1000\n",
      "72/72 [==============================] - 2s 24ms/step - loss: 0.6375 - val_loss: 0.6374\n",
      "Epoch 938/1000\n",
      "72/72 [==============================] - 2s 23ms/step - loss: 0.6375 - val_loss: 0.6375\n",
      "Epoch 939/1000\n",
      "72/72 [==============================] - 2s 24ms/step - loss: 0.6374 - val_loss: 0.6375\n",
      "Epoch 940/1000\n",
      "72/72 [==============================] - 2s 24ms/step - loss: 0.6374 - val_loss: 0.6373\n",
      "Epoch 941/1000\n",
      "72/72 [==============================] - 2s 23ms/step - loss: 0.6374 - val_loss: 0.6373\n",
      "Epoch 942/1000\n",
      "72/72 [==============================] - 2s 23ms/step - loss: 0.6374 - val_loss: 0.6374\n",
      "Epoch 943/1000\n",
      "72/72 [==============================] - 2s 23ms/step - loss: 0.6374 - val_loss: 0.6375\n",
      "Epoch 944/1000\n",
      "72/72 [==============================] - 2s 24ms/step - loss: 0.6374 - val_loss: 0.6372\n",
      "Epoch 945/1000\n",
      "72/72 [==============================] - 2s 24ms/step - loss: 0.6374 - val_loss: 0.6374\n",
      "Epoch 946/1000\n",
      "72/72 [==============================] - 2s 23ms/step - loss: 0.6374 - val_loss: 0.6373\n",
      "Epoch 947/1000\n",
      "72/72 [==============================] - 2s 23ms/step - loss: 0.6374 - val_loss: 0.6373\n",
      "Epoch 948/1000\n",
      "72/72 [==============================] - 2s 24ms/step - loss: 0.6374 - val_loss: 0.6373\n",
      "Epoch 949/1000\n",
      "72/72 [==============================] - 2s 23ms/step - loss: 0.6373 - val_loss: 0.6374\n",
      "Epoch 950/1000\n",
      "72/72 [==============================] - 2s 24ms/step - loss: 0.6373 - val_loss: 0.6372\n",
      "Epoch 951/1000\n",
      "72/72 [==============================] - 2s 24ms/step - loss: 0.6373 - val_loss: 0.6372\n",
      "Epoch 952/1000\n",
      "72/72 [==============================] - 2s 23ms/step - loss: 0.6373 - val_loss: 0.6373\n",
      "Epoch 953/1000\n",
      "72/72 [==============================] - 2s 23ms/step - loss: 0.6373 - val_loss: 0.6372\n",
      "Epoch 954/1000\n",
      "72/72 [==============================] - 2s 23ms/step - loss: 0.6373 - val_loss: 0.6374\n",
      "Epoch 955/1000\n",
      "72/72 [==============================] - 2s 23ms/step - loss: 0.6373 - val_loss: 0.6374\n",
      "Epoch 956/1000\n",
      "72/72 [==============================] - 2s 23ms/step - loss: 0.6373 - val_loss: 0.6376\n",
      "Epoch 957/1000\n",
      "72/72 [==============================] - 2s 23ms/step - loss: 0.6374 - val_loss: 0.6373\n",
      "Epoch 958/1000\n",
      "72/72 [==============================] - 2s 23ms/step - loss: 0.6372 - val_loss: 0.6372\n",
      "Epoch 959/1000\n",
      "72/72 [==============================] - 2s 22ms/step - loss: 0.6373 - val_loss: 0.6372\n",
      "Epoch 960/1000\n",
      "72/72 [==============================] - 2s 24ms/step - loss: 0.6372 - val_loss: 0.6372\n",
      "Epoch 961/1000\n",
      "72/72 [==============================] - 2s 23ms/step - loss: 0.6372 - val_loss: 0.6372\n",
      "Epoch 962/1000\n",
      "72/72 [==============================] - 2s 24ms/step - loss: 0.6372 - val_loss: 0.6371\n",
      "Epoch 963/1000\n",
      "72/72 [==============================] - 2s 23ms/step - loss: 0.6372 - val_loss: 0.6372\n",
      "Epoch 964/1000\n",
      "72/72 [==============================] - 2s 24ms/step - loss: 0.6372 - val_loss: 0.6372\n",
      "Epoch 965/1000\n",
      "72/72 [==============================] - 2s 23ms/step - loss: 0.6372 - val_loss: 0.6371\n",
      "Epoch 966/1000\n",
      "72/72 [==============================] - 2s 24ms/step - loss: 0.6372 - val_loss: 0.6372\n",
      "Epoch 967/1000\n",
      "72/72 [==============================] - 2s 24ms/step - loss: 0.6372 - val_loss: 0.6373\n",
      "Epoch 968/1000\n",
      "72/72 [==============================] - 2s 23ms/step - loss: 0.6372 - val_loss: 0.6371\n",
      "Epoch 969/1000\n",
      "72/72 [==============================] - 2s 24ms/step - loss: 0.6372 - val_loss: 0.6372\n",
      "Epoch 970/1000\n",
      "72/72 [==============================] - 2s 24ms/step - loss: 0.6372 - val_loss: 0.6373\n",
      "Epoch 971/1000\n",
      "72/72 [==============================] - 2s 24ms/step - loss: 0.6371 - val_loss: 0.6372\n",
      "Epoch 972/1000\n",
      "72/72 [==============================] - 2s 24ms/step - loss: 0.6372 - val_loss: 0.6371\n",
      "Epoch 973/1000\n",
      "72/72 [==============================] - 2s 23ms/step - loss: 0.6372 - val_loss: 0.6375\n",
      "Epoch 974/1000\n",
      "72/72 [==============================] - 2s 23ms/step - loss: 0.6372 - val_loss: 0.6371\n",
      "Epoch 975/1000\n",
      "72/72 [==============================] - 2s 23ms/step - loss: 0.6373 - val_loss: 0.6372\n",
      "Epoch 976/1000\n",
      "72/72 [==============================] - 2s 23ms/step - loss: 0.6372 - val_loss: 0.6371\n",
      "Epoch 977/1000\n",
      "72/72 [==============================] - 2s 23ms/step - loss: 0.6371 - val_loss: 0.6371\n",
      "Epoch 978/1000\n",
      "72/72 [==============================] - 2s 23ms/step - loss: 0.6371 - val_loss: 0.6371\n",
      "Epoch 979/1000\n",
      "72/72 [==============================] - 2s 24ms/step - loss: 0.6371 - val_loss: 0.6371\n",
      "Epoch 980/1000\n",
      "72/72 [==============================] - 2s 24ms/step - loss: 0.6371 - val_loss: 0.6371\n",
      "Epoch 981/1000\n",
      "72/72 [==============================] - 2s 24ms/step - loss: 0.6371 - val_loss: 0.6371\n",
      "Epoch 982/1000\n",
      "72/72 [==============================] - 2s 24ms/step - loss: 0.6371 - val_loss: 0.6370\n",
      "Epoch 983/1000\n",
      "72/72 [==============================] - 2s 25ms/step - loss: 0.6371 - val_loss: 0.6371\n",
      "Epoch 984/1000\n",
      "72/72 [==============================] - 2s 24ms/step - loss: 0.6371 - val_loss: 0.6370\n",
      "Epoch 985/1000\n",
      "72/72 [==============================] - 2s 25ms/step - loss: 0.6371 - val_loss: 0.6371\n",
      "Epoch 986/1000\n",
      "72/72 [==============================] - 2s 24ms/step - loss: 0.6371 - val_loss: 0.6370\n",
      "Epoch 987/1000\n",
      "72/72 [==============================] - 2s 25ms/step - loss: 0.6371 - val_loss: 0.6370\n",
      "Epoch 988/1000\n",
      "72/72 [==============================] - 2s 24ms/step - loss: 0.6371 - val_loss: 0.6370\n",
      "Epoch 989/1000\n",
      "72/72 [==============================] - 2s 23ms/step - loss: 0.6371 - val_loss: 0.6370\n",
      "Epoch 990/1000\n",
      "72/72 [==============================] - 2s 24ms/step - loss: 0.6371 - val_loss: 0.6370\n",
      "Epoch 991/1000\n",
      "72/72 [==============================] - 2s 24ms/step - loss: 0.6371 - val_loss: 0.6370\n",
      "Epoch 992/1000\n",
      "72/72 [==============================] - 2s 24ms/step - loss: 0.6371 - val_loss: 0.6370\n",
      "Epoch 993/1000\n",
      "72/72 [==============================] - 2s 24ms/step - loss: 0.6371 - val_loss: 0.6371\n",
      "Epoch 994/1000\n",
      "72/72 [==============================] - 2s 24ms/step - loss: 0.6371 - val_loss: 0.6370\n",
      "Epoch 995/1000\n",
      "72/72 [==============================] - 2s 24ms/step - loss: 0.6370 - val_loss: 0.6370\n",
      "Epoch 996/1000\n",
      "72/72 [==============================] - 2s 25ms/step - loss: 0.6370 - val_loss: 0.6371\n",
      "Epoch 997/1000\n",
      "72/72 [==============================] - 2s 24ms/step - loss: 0.6370 - val_loss: 0.6370\n",
      "Epoch 998/1000\n",
      "72/72 [==============================] - 2s 25ms/step - loss: 0.6370 - val_loss: 0.6369\n",
      "Epoch 999/1000\n",
      "72/72 [==============================] - 2s 24ms/step - loss: 0.6371 - val_loss: 0.6371\n",
      "Epoch 1000/1000\n",
      "72/72 [==============================] - 2s 24ms/step - loss: 0.6370 - val_loss: 0.6369\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f902c6a04f0>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tensorflow.python.keras.callbacks import TensorBoard\n",
    "\n",
    "autoencoder.fit(train, train,\n",
    "                epochs=1000,\n",
    "                batch_size=128,\n",
    "                shuffle=True,\n",
    "                validation_data=(train, train),\n",
    "                callbacks=[TensorBoard(log_dir='/tmp/autoencoder')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-30 00:36:00.949234: W tensorflow/python/util/util.cc:348] Sets are not currently considered sequences, but this may change in the future, so consider avoiding using them.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: autoencoder_cartoon_fc/assets\n"
     ]
    }
   ],
   "source": [
    "autoencoder.save(\"autoencoder_cartoon_fc\")\n",
    "autoencoder.save_weights(\"autoencoder_cartoon_fc_weights.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "autoencoder = keras.models.load_model('autoencoder_cartoon')\n",
    "autoencoder.load_weights(\"autoencoder_cartoon_weights.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "def plot_AE_history(history):\n",
    "    plt.plot(history.history.history['loss'],\n",
    "             'b',\n",
    "             label='Training loss')\n",
    "    plt.plot(history.history.history['val_loss'],\n",
    "             'r',\n",
    "             label='Validation loss')\n",
    "    plt.legend(loc='lower right')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss, [mse]')\n",
    "    plt.ylim([0,0.7])\n",
    "    plt.show()\n",
    "\n",
    "plot_AE_history(autoencoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "X_pred = autoencoder.predict(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "normal_max = 0\n",
    "loss_list = []\n",
    "for i in range(train.shape[0]):\n",
    "    normal_loss = X_pred[i]\n",
    "    loss = np.mean((normal_loss-train[i])**2)\n",
    "    normal_max = max(normal_max, loss)\n",
    "    loss_list.append(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.collections.PathCollection at 0x7f9000636e80>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "a = np.linspace(0, 9200, 9200)\n",
    "plt.scatter(a, np.array(loss_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "X_test_pred = autoencoder.predict(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "threshold = np.linspace(0, 1, 5000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "losses = []\n",
    "for i in range(test.shape[0]):\n",
    "    x_decoded = X_test_pred[i]\n",
    "    loss = np.mean((x_decoded-test[i])**2)\n",
    "    losses.append(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.collections.PathCollection at 0x7f900078a460>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "a = np.linspace(0, 800, 800)\n",
    "plt.scatter(a, np.array(losses[0:800]), color = 'red')\n",
    "b = np.linspace(800, 1000, 200)\n",
    "plt.scatter(b, np.array(losses[800:]), color = 'blue')  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score,f1_score\n",
    "acc_list = []\n",
    "f1_list = []\n",
    "for t in threshold:    \n",
    "    y_pred = (losses>t).astype(np.int64)\n",
    "    acc_list.append(accuracy_score(y_pred,test_y))\n",
    "    f1_list.append(f1_score(y_pred,test_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x7ff4bc6f87f0>"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(8,6))\n",
    "plt.plot(threshold,acc_list,c = 'y',label = 'acc')\n",
    "plt.plot(threshold,f1_list,c = 'b',label = 'f1')\n",
    "plt.xlabel('threshold')\n",
    "plt.ylabel('classification score')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recommended threshold: 0.063, related f1 score: 0.965\n",
      "In 1000 data of test set, TP: 194, FN: 6, FP: 8, TN: 792\n"
     ]
    }
   ],
   "source": [
    "i = np.argmax(f1_list)\n",
    "t = threshold[i]\n",
    "score = f1_list[i]\n",
    "print('Recommended threshold: %.3f, related f1 score: %.3f'%(t,score))\n",
    "\n",
    "y_pred = (losses>t).astype(np.int64)\n",
    "y_pred = y_pred.reshape(1000,1)\n",
    "test_y = test_y.reshape(1000,1)\n",
    "TP = ((test_y==1) & (y_pred==1)).sum()\n",
    "FN = ((test_y==1) & (y_pred==0)).sum()\n",
    "FP = ((test_y==0) & (y_pred==1)).sum()\n",
    "TN = ((test_y==0) & (y_pred==0)).sum()\n",
    "print('In %d data of test set, TP: %d, FN: %d, FP: %d, TN: %d'%(len(test_y),TP,FN,FP,TN))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.97"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TP/(TP+FN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recommended threshold: 0.063, related acc score: 0.986\n",
      "In 1000 data of test set, TP: 194, FN: 6, FP: 8, TN: 792\n"
     ]
    }
   ],
   "source": [
    "i = np.argmax(acc_list)\n",
    "t = threshold[i]\n",
    "score = acc_list[i]\n",
    "print('Recommended threshold: %.3f, related acc score: %.3f'%(t,score))\n",
    "\n",
    "y_pred = (losses>t).astype(np.int64)\n",
    "y_pred = y_pred.reshape(1000,1)\n",
    "test_y = test_y.reshape(1000,1)\n",
    "TP = ((test_y==1) & (y_pred==1)).sum()\n",
    "FN = ((test_y==1) & (y_pred==0)).sum()\n",
    "FP = ((test_y==0) & (y_pred==1)).sum()\n",
    "TN = ((test_y==0) & (y_pred==0)).sum()\n",
    "print('In %d data of test set, TP: %d, FN: %d, FP: %d, TN: %d'%(len(test_y),TP,FN,FP,TN))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.97"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TP/(TP+FN)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "ebcc9ab275a0b8333d76f3c9007aff6798cc23ea07d3b4c53c0b1df0392e66fe"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}